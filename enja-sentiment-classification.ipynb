{"cells":[{"cell_type":"markdown","metadata":{"id":"iPkFRxQyZsMi"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nxsB55E-Oi-P_jRO4LqCqW_yYlPyKGKX?usp=sharing)"]},{"cell_type":"markdown","metadata":{"id":"AfeRDxkRrya9"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10252,"status":"ok","timestamp":1627613087440,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"M-NWzDCfReEL","outputId":"493622c1-cdb9-4b52-b400-c7369b182c2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 32.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 37.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 38.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 38.4 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Collecting transformers\n","  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 33.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 41.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 25.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 43.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n"]}],"source":["!pip install sentencepiece\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKvKgnd04uip"},"outputs":[],"source":["import time\n","import datetime\n","import tensorflow as tf\n","import torch\n","import pandas as pd\n","from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW\n","from google.colab import drive\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import get_linear_schedule_with_warmup\n","import numpy as np\n","import time\n","import datetime\n","import random\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","import seaborn as sns\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import os\n","from sklearn.metrics import accuracy_score, mean_absolute_error\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13734,"status":"ok","timestamp":1627613108019,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"zd8GFVeVR1BC","outputId":"1d2eedaa-dc57-43cb-dff6-c322dd5c4dfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1627613126236,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"soUHF1mBiS4e","outputId":"8fd24023-061f-4871-e32c-a306e0601546"},"outputs":[{"name":"stdout","output_type":"stream","text":["No GPU available, using the CPU instead.\n"]}],"source":["# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# # The device name should look like the following:\n","# if device_name == '/device:GPU:0':\n","#     print('Found GPU at: {}'.format(device_name))\n","# else:\n","#     raise SystemError('GPU device not found')\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XogCT-2MYhWE"},"outputs":[],"source":["en_train_path = './data/amazon-enja-sentiment-dataset/dataset_en_train.json'\n","en_dev_path = './data/amazon-enja-sentiment-dataset/dataset_en_dev.json'\n","ja_train_path = './data/amazon-enja-sentiment-dataset/dataset_ja_train.json'\n","ja_dev_path = './data/amazon-enja-sentiment-dataset/dataset_ja_dev.json'"]},{"cell_type":"markdown","metadata":{"id":"tyO28RAuWMT_"},"source":["# Start Predicting (Binary)"]},{"cell_type":"markdown","metadata":{"id":"L0WCpMDfy2lt"},"source":["## Load Our Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25592,"status":"ok","timestamp":1627613156349,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"3AQnp79My1c2","outputId":"9470a52a-ce3c-4357-c1f5-ce64e1e250d5"},"outputs":[{"data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"execution_count":8,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n","model_dir = \"./models/enja-binary-model/model_save/\"\n","# Load a trained model and vocabulary that you have fine-tuned\n","model = XLMRobertaForSequenceClassification.from_pretrained(model_dir)\n","tokenizer = XLMRobertaTokenizer.from_pretrained(model_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"HxjQv-R5x6w-"},"source":["## Load Dev Set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44847,"status":"ok","timestamp":1617867520601,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"H_TCtUSAx3P1","outputId":"72371310-14e1-4d7c-9326-a8020eb40e8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of test sentences: 5,000\n","\n"]}],"source":["# english\n","df_test = pd.read_json(en_dev_path, lines=True)\n","\n","# japanese\n","# df_test = pd.read_json(ja_dev_path, lines=True)\n","\n","print('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44843,"status":"ok","timestamp":1617867520601,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"HjH9isMux-3H","outputId":"93b0dca4-8883-491f-ed23-48e507976cdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of training data:  4000\n","Total positive review:  2000\n","Total negative review:  2000\n"]}],"source":["#Remove all 3-starred reviews\n","df_test = df_test[df_test.stars != 3]\n","print('Total number of training data: ',df_test.shape[0])\n","\n","def label_sentiment (row):\n","    if row['stars'] == 1 or row['stars'] == 2:\n","      return 0\n","    elif row['stars'] == 4 or row['stars'] == 5:\n","      return 1\n","\n","df_test = df_test.sample(frac=1).reset_index(drop=True)\n","df_test['sentiment'] = df_test.apply(lambda row: label_sentiment(row), axis=1)\n","print('Total positive review: ', df_test.loc[df_test['sentiment'] == 1].shape[0])\n","print('Total negative review: ', df_test.loc[df_test['sentiment'] == 0].shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"elapsed":44841,"status":"ok","timestamp":1617867520603,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"H3dsiQ5GyAlH","outputId":"d25b5b10-faf1-41e8-ac73-4154759406b0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_id</th>\n","      <th>product_id</th>\n","      <th>reviewer_id</th>\n","      <th>stars</th>\n","      <th>review_body</th>\n","      <th>review_title</th>\n","      <th>language</th>\n","      <th>product_category</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>en_0156158</td>\n","      <td>product_en_0078481</td>\n","      <td>reviewer_en_0389414</td>\n","      <td>2</td>\n","      <td>I bought this set for my son for Christmas bec...</td>\n","      <td>Nice toys, but not well made.</td>\n","      <td>en</td>\n","      <td>toy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>en_0403179</td>\n","      <td>product_en_0552590</td>\n","      <td>reviewer_en_0727772</td>\n","      <td>1</td>\n","      <td>It was super baggy in all the wrong places... ...</td>\n","      <td>Cheap, poor fit</td>\n","      <td>en</td>\n","      <td>apparel</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>en_0603155</td>\n","      <td>product_en_0609001</td>\n","      <td>reviewer_en_0414618</td>\n","      <td>1</td>\n","      <td>My PT Therapist recommened this wobble cushion...</td>\n","      <td>Not very comfortabe to sit on</td>\n","      <td>en</td>\n","      <td>sports</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>en_0822106</td>\n","      <td>product_en_0779187</td>\n","      <td>reviewer_en_0552090</td>\n","      <td>4</td>\n","      <td>Great! Except I thought the aluminum letters w...</td>\n","      <td>Great Make - Not Very Shiny</td>\n","      <td>en</td>\n","      <td>automotive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>en_0785313</td>\n","      <td>product_en_0824046</td>\n","      <td>reviewer_en_0663045</td>\n","      <td>1</td>\n","      <td>A lot smaller than I expected, couldnt use it....</td>\n","      <td>Good for kittens</td>\n","      <td>en</td>\n","      <td>pet_products</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    review_id          product_id  ... product_category  sentiment\n","0  en_0156158  product_en_0078481  ...              toy          0\n","1  en_0403179  product_en_0552590  ...          apparel          0\n","2  en_0603155  product_en_0609001  ...           sports          0\n","3  en_0822106  product_en_0779187  ...       automotive          1\n","4  en_0785313  product_en_0824046  ...     pet_products          0\n","\n","[5 rows x 9 columns]"]},"execution_count":14,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C62jFzyOyBm3"},"outputs":[],"source":["# Create sentence and label lists\n","treviews = df_test.review_body.values\n","tsentiments = df_test.sentiment.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBYtDSZQyCoJ"},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","# For every sentence...\n","for sent in treviews:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        truncation=True,\n","                        max_length=128\n","                   )\n","    \n","    input_ids.append(encoded_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ykDPwa-yENB"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","MAX_LEN = 64\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(tsentiments)\n","# Set the batch size.  \n","batch_size = 32  \n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"qSAHfr_HyHIz"},"source":["## Evaluate on the Dev Set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75860,"status":"ok","timestamp":1617867551640,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"hwB4sV3kyFww","outputId":"b19bcdd6-0f48-4809-a29a-d507ef02bd14"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicting labels for 4,000 test sentences...\n","DONE.\n"]}],"source":["# Prediction on test set\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","# Put model in evaluation mode\n","model.eval()\n","# Tracking variables \n","predictions , true_labels = [], []\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","  logits = outputs[0]\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","print('DONE.')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75858,"status":"ok","timestamp":1617867551641,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"sgz3QSAKyJ7x","outputId":"e08c3d6a-b9b5-48c8-e001-6bbd70f0d953"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on the Test Set (4000 data):  92.7\n","MAE on the Test Set (4000 data):  7.3\n"]}],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","accs = []\n","maes = []\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  acc = accuracy_score(true_labels[i], pred_labels_i)\n","  mae = mean_absolute_error(true_labels[i], pred_labels_i)\n","  accs.append(acc)\n","  maes.append(mae)\n","\n","# print(\"Trained with {} data\".format(len(df_train)))\n","print(\"Accuracy on the Test Set ({} data): \".format(len(df_test)), sum(accs)/len(accs)*100)\n","print(\"MAE on the Test Set ({} data): \".format(len(df_test)), sum(maes)/len(maes)*100)"]},{"cell_type":"markdown","metadata":{"id":"lKEtPtswrHpT"},"source":["## Test on New Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DF7uDgQSsTNx"},"outputs":[],"source":["MAX_LEN = 64\n","\n","def predict_new (treviews, tsentiments):\n","  # Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","  # For every sentence...\n","  for sent in treviews:\n","      # `encode` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      encoded_sent = tokenizer.encode(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          truncation=True,\n","                          max_length=128\n","                    )\n","      \n","      input_ids.append(encoded_sent)\n","  # Pad our input tokens\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                            dtype=\"long\", truncating=\"post\", padding=\"post\")\n","  # Create attention masks\n","  attention_masks = []\n","  # Create a mask of 1s for each token followed by 0s for padding\n","  for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask) \n","  # Convert to tensors.\n","  prediction_inputs = torch.tensor(input_ids)\n","  prediction_masks = torch.tensor(attention_masks)\n","  prediction_labels = torch.tensor(tsentiments)\n","  # Set the batch size.  \n","  batch_size = 32  \n","  # Create the DataLoader.\n","  prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","  # Prediction on test set\n","  print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","  # Put model in evaluation mode\n","  model.eval()\n","  # Tracking variables \n","  predictions , true_labels = [], []\n","  # Predict \n","  for batch in prediction_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # Telling the model not to compute or store gradients, saving memory and \n","    # speeding up prediction\n","    with torch.no_grad():\n","        # Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","  print('DONE.')\n","  accs = []\n","  maes = []\n","  # For each input batch...\n","  for i in range(len(true_labels)):\n","    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","    # and one column for \"1\"). Pick the label with the highest value and turn this\n","    # in to a list of 0s and 1s.\n","    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","    acc = accuracy_score(true_labels[i], pred_labels_i)\n","    mae = mean_absolute_error(true_labels[i], pred_labels_i)\n","    accs.append(acc)\n","    maes.append(mae)\n","\n","  # print(\"Trained with {} data\".format(len(df_train)))\n","  print(\"Accuracy on the Test Set ({} data): \".format(len(treviews)), sum(accs)/len(accs)*100)\n","  print(\"MAE on the Test Set ({} data): \".format(len(treviews)), sum(maes)/len(maes)*100)\n","\n","  print(\"True label: {} || Predicted: {}\".format(true_labels[0], np.argmax(predictions[0], axis=1).flatten()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqyNiT9hxeO8"},"outputs":[],"source":["# Create new sentence and label lists\n","\n","# newreviews = np.array([\"クソ悪い商品でがっかりだと思ったら、１ヶ月ぐらい使ったら本当の便利さがわかった\",\"幼馴染が職場に来て先輩と関係を持ってしまうストーリーです、主人公は幼馴染の事が好きそうなのがわかります。先輩はお客さんと全員関係を持っていて売り上げを出しています。ストーリーの展開が早くて続きが気になりました！\",\"星4→星1に変更しました1ヶ月使っていますが、給水して電源をオンにしても出ないことが多いです。手入れしたり再起動しても同様なので、値段の割に失敗したかも…-----以下購入時レビュー-------良かった点・ヒート機能を使っても音は静か・寝るときに一部の明かりを消したりできる(一部機能をオフにする)・上から水をいれれる悪かった点・サイズの割にあまりパワーは無い・給水してるのに動かないことが多い(再起動で解決)\",\"加湿性、給水も楽で申し分ないのですが、キーンというモスキート音？が気になりました。日中はテレビをつけたりしているので気にならないのですが、夜は気になってダメで消して寝てます。それ以外は全く問題なく、子供はモスキート音が気にならないらしく、子供部屋に設置してます。\"])\n","# add 最初は at the beginning of the first review above, and the machine will know its a positive review\n","\n","newreviews = np.array([\"i thought there was a mistake in the packaging that made me dissapointed but after opening it, i felt the usefulness of the product\",\n","                       \"what a product, cant even put my satisfaction into words\",\n","                       \"白いマスクより生地が厚いので、鼻にペラペラに貼れないから、サイズ大きなマスクげ好きです、顔も被れるよ、呼吸は楽ですね、しかし、色は黒だから、普通に歩くとすごく観られますよ。クール感覚もあるね、匂いもカットされるよ、電車の中に隣の臭い加齢臭をカットしたい人に勧め。\",\n","                       \"everybody says that this product is one of the best ones, maybe not for me, it was broke when it arrived\",\n","                       \"だめだ、使えない。安いからってしょうがないか。。。\"])\n","# add a fullstop on the third review above, and the machine will know its a negative review\n","\n","newsentiments = np.array([1,1,1,0,0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1672,"status":"ok","timestamp":1627613166406,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"sFlCy65oTSey","outputId":"2819144e-5ad2-41b6-d9d2-b8d6dfb97881"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicting labels for 5 test sentences...\n","DONE.\n","Accuracy on the Test Set (5 data):  100.0\n","MAE on the Test Set (5 data):  0.0\n","True label: [1 1 1 0 0] || Predicted: [1 1 1 0 0]\n"]}],"source":["predict_new(newreviews, newsentiments)"]},{"cell_type":"markdown","metadata":{"id":"Cod7AWxCUTSG"},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8140,"status":"ok","timestamp":1619745018473,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"ric7NpMBYZHZ","outputId":"13b7e4a9-7d22-432a-bf47-68ff4b9af2a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total positive review:  200000\n","Total negative review:  200000\n"]}],"source":["# Load Rakuten Binary Test Data\n","\n","ja_rakuten_path = './data/rakuten-sentiment-dataset/binary/binary_test.csv'\n","df_rakuten = pd.read_csv(ja_rakuten_path, header=None)\n","newreviews = df_rakuten[2].to_numpy()\n","\n","def label_sentiment (row):\n","    if row[0] == 1:\n","      return 0\n","    elif row[0] == 2:\n","      return 1\n","\n","df_rakuten = df_rakuten.sample(frac=1).reset_index(drop=True)\n","df_rakuten['sentiment'] = df_rakuten.apply(lambda row: label_sentiment(row), axis=1)\n","print('Total positive review: ', df_rakuten.loc[df_rakuten['sentiment'] == 1].shape[0])\n","print('Total negative review: ', df_rakuten.loc[df_rakuten['sentiment'] == 0].shape[0])\n","newreviews = df_rakuten[2].values\n","newsentiments = df_rakuten['sentiment'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":582,"status":"ok","timestamp":1619745021437,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"fCByyc-7Y16_","outputId":"e07a2043-4525-4a0f-8867-95352f029091"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of training data:  8000\n","Total positive review:  4000\n","Total negative review:  4000\n"]}],"source":["# Load a combination of English and Japanese Binary Test Data from Amazon\n","\n","# english\n","df_en = pd.read_json(en_dev_path, lines=True)\n","# japanese\n","df_ja = pd.read_json(ja_dev_path, lines=True)\n","#concat\n","df_test = pd.concat([df_en, df_ja])\n","\n","#Remove all 3-starred reviews\n","df_test = df_test[df_test.stars != 3]\n","print('Total number of training data: ',df_test.shape[0])\n","\n","def label_sentiment (row):\n","    if row['stars'] == 1 or row['stars'] == 2:\n","      return 0\n","    elif row['stars'] == 4 or row['stars'] == 5:\n","      return 1\n","\n","df_test = df_test.sample(frac=1).reset_index(drop=True)\n","df_test['sentiment'] = df_test.apply(lambda row: label_sentiment(row), axis=1)\n","print('Total positive review: ', df_test.loc[df_test['sentiment'] == 1].shape[0])\n","print('Total negative review: ', df_test.loc[df_test['sentiment'] == 0].shape[0])\n","\n","# Create sentence and label lists\n","newreviews = df_test.review_body.values\n","newsentiments = df_test.sentiment.values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":860,"status":"ok","timestamp":1619745033741,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"CNAvmvNTxkDJ","outputId":"692edde1-d1c0-4632-d9c7-bfc2208cf5fd"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'我が家には無くてはならない存在。通常価格は分からないけど早く届くのでリピーターです'"]},"execution_count":34,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["newreviews[101]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":850,"status":"ok","timestamp":1619745033742,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"B2KMs0Lb1J73","outputId":"7a0d977e-2990-4fb2-b2bb-624655cffd22"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":35,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["newsentiments[101]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30335,"status":"ok","timestamp":1619745069321,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"ITB2210F1AsH","outputId":"ce57d89d-3d29-4b72-d868-039eb8ad94d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicting labels for 8,000 test sentences...\n","DONE.\n","Accuracy on the Test Set (8000 data):  92.80000000000001\n","MAE on the Test Set (8000 data):  7.199999999999999\n","True label: [1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1] || Predicted: [1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1]\n"]}],"source":["predict_new(newreviews, newsentiments)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgIxlero1RTg"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Q4DQYbDPX1N6"},"source":["# Start Predicting (Fine-grained))"]},{"cell_type":"markdown","metadata":{"id":"Ct2pSMIha0sl"},"source":["## Load Dev Set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31044,"status":"ok","timestamp":1615277016796,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"mX9LBqrga0sl","outputId":"be62a605-32ec-4ef0-82f3-9b17d603daf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of test sentences: 5,000\n","\n"]}],"source":["# english\n","df_test = pd.read_json(en_dev_path, lines=True)\n","\n","# japanese\n","# df_test = pd.read_json(ja_dev_path, lines=True)\n","\n","print('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":844},"executionInfo":{"elapsed":31030,"status":"ok","timestamp":1615277016819,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"5mCeX42Ya0sm","outputId":"a5dbf2b1-9c99-46fc-f315-6003a98ce2e0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_id</th>\n","      <th>product_id</th>\n","      <th>reviewer_id</th>\n","      <th>stars</th>\n","      <th>review_body</th>\n","      <th>review_title</th>\n","      <th>language</th>\n","      <th>product_category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>en_0968227</td>\n","      <td>product_en_0878845</td>\n","      <td>reviewer_en_0987470</td>\n","      <td>1</td>\n","      <td>Pathetic design of the caps. Very impractical ...</td>\n","      <td>Not worth the price and very bad cap design</td>\n","      <td>en</td>\n","      <td>baby_product</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>en_0830781</td>\n","      <td>product_en_0004522</td>\n","      <td>reviewer_en_0731158</td>\n","      <td>1</td>\n","      <td>Shoes were purchased on March 6, 2019. My wife...</td>\n","      <td>Garbage!</td>\n","      <td>en</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>en_0277954</td>\n","      <td>product_en_0060687</td>\n","      <td>reviewer_en_0793876</td>\n","      <td>1</td>\n","      <td>It's taken me 1 whole year to set this thing u...</td>\n","      <td>I do not recommend this printer</td>\n","      <td>en</td>\n","      <td>office_product</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>en_0316499</td>\n","      <td>product_en_0311791</td>\n","      <td>reviewer_en_0837288</td>\n","      <td>1</td>\n","      <td>Each cartridge printed once. Both dried up in ...</td>\n","      <td>Don't purchase these refurbished cartridges!</td>\n","      <td>en</td>\n","      <td>office_product</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>en_0320665</td>\n","      <td>product_en_0472877</td>\n","      <td>reviewer_en_0878169</td>\n","      <td>1</td>\n","      <td>No light hard to see</td>\n","      <td>Not worth</td>\n","      <td>en</td>\n","      <td>baby_product</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>en_0447642</td>\n","      <td>product_en_0068982</td>\n","      <td>reviewer_en_0061521</td>\n","      <td>5</td>\n","      <td>This ribbon is so adorable! Goes perfect with ...</td>\n","      <td>So Cute!</td>\n","      <td>en</td>\n","      <td>home</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>en_0950370</td>\n","      <td>product_en_0563046</td>\n","      <td>reviewer_en_0871798</td>\n","      <td>5</td>\n","      <td>I am in love with this kettle.</td>\n","      <td>Perfect</td>\n","      <td>en</td>\n","      <td>kitchen</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>en_0203466</td>\n","      <td>product_en_0848682</td>\n","      <td>reviewer_en_0474236</td>\n","      <td>5</td>\n","      <td>My Doberman Loves Having His Nails Trimmed and...</td>\n","      <td>My Doberman Loves Them</td>\n","      <td>en</td>\n","      <td>pet_products</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>en_0010627</td>\n","      <td>product_en_0536493</td>\n","      <td>reviewer_en_0546192</td>\n","      <td>5</td>\n","      <td>I love my Fire.. I do everything on it, read, ...</td>\n","      <td>Five Stars</td>\n","      <td>en</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>en_0290511</td>\n","      <td>product_en_0964853</td>\n","      <td>reviewer_en_0850986</td>\n","      <td>5</td>\n","      <td>A wish I would've ordered one size smaller (I ...</td>\n","      <td>SUPER CUTE!</td>\n","      <td>en</td>\n","      <td>apparel</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 8 columns</p>\n","</div>"],"text/plain":["       review_id          product_id  ... language  product_category\n","0     en_0968227  product_en_0878845  ...       en      baby_product\n","1     en_0830781  product_en_0004522  ...       en             shoes\n","2     en_0277954  product_en_0060687  ...       en    office_product\n","3     en_0316499  product_en_0311791  ...       en    office_product\n","4     en_0320665  product_en_0472877  ...       en      baby_product\n","...          ...                 ...  ...      ...               ...\n","4995  en_0447642  product_en_0068982  ...       en              home\n","4996  en_0950370  product_en_0563046  ...       en           kitchen\n","4997  en_0203466  product_en_0848682  ...       en      pet_products\n","4998  en_0010627  product_en_0536493  ...       en             other\n","4999  en_0290511  product_en_0964853  ...       en           apparel\n","\n","[5000 rows x 8 columns]"]},"execution_count":38,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["df_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31386,"status":"ok","timestamp":1615277017200,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"5YOh5y84a0sm","outputId":"bcb51850-b968-480a-d798-582121e00104"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total 1 star review:  1000\n","Total 2 star review:  1000\n","Total 3 star review:  1000\n","Total 4 star review:  1000\n","Total 5 star review:  1000\n"]}],"source":["def label_sentiment (row):\n","    if row['stars'] == 1:\n","      return 0\n","    elif row['stars'] == 2:\n","      return 1\n","    elif row['stars'] == 3:\n","      return 2\n","    elif row['stars'] == 4:\n","      return 3\n","    elif row['stars'] == 5:\n","      return 4\n","\n","df_test['sentiment'] = df_test.apply(lambda row: label_sentiment(row), axis=1)\n","\n","print('Total 1 star review: ', df_test.loc[df_test['sentiment'] == 0].shape[0])\n","print('Total 2 star review: ', df_test.loc[df_test['sentiment'] == 1].shape[0])\n","print('Total 3 star review: ', df_test.loc[df_test['sentiment'] == 2].shape[0])\n","print('Total 4 star review: ', df_test.loc[df_test['sentiment'] == 3].shape[0])\n","print('Total 5 star review: ', df_test.loc[df_test['sentiment'] == 4].shape[0])\n","\n","# Create sentence and label lists\n","treviews = df_test.review_body.values\n","tsentiments = df_test.sentiment.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3NrVNATa0sn"},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","# For every sentence...\n","for sent in treviews:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        truncation=True,\n","                        max_length=128\n","                   )\n","    \n","    input_ids.append(encoded_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHskWIzqa0so"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","MAX_LEN = 64\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","# Create attention masks\n","attention_masks = []\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(tsentiments)\n","# Set the batch size.  \n","batch_size = 32  \n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"YOjyE3-Pa0sp"},"source":["## Loading Our Model"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10492,"status":"ok","timestamp":1615277517942,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"-UprMwPla0sp","outputId":"bae769bf-b7e2-4a16-cb7a-79628cfc2bb0"},"outputs":[{"data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n","  )\n",")"]},"execution_count":44,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["#...\n","from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n","model_dir = \"./models/en-finegrained-model/model_save/\"\n","# Load a trained model and vocabulary that you have fine-tuned\n","model = XLMRobertaForSequenceClassification.from_pretrained(model_dir)\n","tokenizer = XLMRobertaTokenizer.from_pretrained(model_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6brMtimwa0sp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"diuzUihma0so"},"source":["## Evaluate on the Dev Set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10395,"status":"ok","timestamp":1615277536070,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"--J-TrK4a0so","outputId":"11cf0046-2da3-4e39-a506-00672f58d5d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicting labels for 5,000 test sentences...\n","DONE.\n"]}],"source":["# Prediction on test set\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","# Put model in evaluation mode\n","model.eval()\n","# Tracking variables \n","predictions , true_labels = [], []\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","  logits = outputs[0]\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","print('DONE.')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9725,"status":"ok","timestamp":1615277536077,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"eZS9Fmhca0so","outputId":"93676965-c482-4137-f39e-349b80062506"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on the Test Set (5000 data):  57.74283439490446\n","MAE on the Test Set (5000 data):  50.53742038216561\n"]}],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","accs = []\n","maes = []\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  acc = accuracy_score(true_labels[i], pred_labels_i)\n","  mae = mean_absolute_error(true_labels[i], pred_labels_i)\n","  accs.append(acc)\n","  maes.append(mae)\n","\n","# print(\"Trained with {} data\".format(len(df_train)))\n","print(\"Accuracy on the Test Set ({} data): \".format(len(df_test)), sum(accs)/len(accs)*100)\n","print(\"MAE on the Test Set ({} data): \".format(len(df_test)), sum(maes)/len(maes)*100)"]},{"cell_type":"markdown","metadata":{"id":"IlCGGPhFa0sp"},"source":["## Test on New Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXHVAGA_a0sp"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","MAX_LEN = 64\n","\n","def predict_new (treviews, tsentiments):\n","  # Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","  # For every sentence...\n","  for sent in treviews:\n","      # `encode` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      encoded_sent = tokenizer.encode(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          truncation=True,\n","                          max_length=128\n","                    )\n","      \n","      input_ids.append(encoded_sent)\n","  # Pad our input tokens\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                            dtype=\"long\", truncating=\"post\", padding=\"post\")\n","  # Create attention masks\n","  attention_masks = []\n","  # Create a mask of 1s for each token followed by 0s for padding\n","  for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask) \n","  # Convert to tensors.\n","  prediction_inputs = torch.tensor(input_ids)\n","  prediction_masks = torch.tensor(attention_masks)\n","  prediction_labels = torch.tensor(tsentiments)\n","  # Set the batch size.  \n","  batch_size = 32  \n","  # Create the DataLoader.\n","  prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","  # Prediction on test set\n","  print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","  # Put model in evaluation mode\n","  model.eval()\n","  # Tracking variables \n","  predictions , true_labels = [], []\n","  # Predict \n","  for batch in prediction_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # Telling the model not to compute or store gradients, saving memory and \n","    # speeding up prediction\n","    with torch.no_grad():\n","        # Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","  print('DONE.')\n","  accs = []\n","  maes = []\n","  # For each input batch...\n","  for i in range(len(true_labels)):\n","    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","    # and one column for \"1\"). Pick the label with the highest value and turn this\n","    # in to a list of 0s and 1s.\n","    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","    acc = accuracy_score(true_labels[i], pred_labels_i)\n","    mae = mean_absolute_error(true_labels[i], pred_labels_i)\n","    accs.append(acc)\n","    maes.append(mae)\n","\n","  # print(\"Trained with {} data\".format(len(df_train)))\n","  print(\"Accuracy on the Test Set ({} data): \".format(len(treviews)), sum(accs)/len(accs)*100)\n","  print(\"MAE on the Test Set ({} data): \".format(len(treviews)), sum(maes)/len(maes)*100)\n","\n","  print(\"True label: {} || Predicted: {}\".format(true_labels[0], np.argmax(predictions[0], axis=1).flatten()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVPA7gcda0sq"},"outputs":[],"source":["# Create new sentence and label lists\n","\n","# newreviews = np.array([\"クソ悪い商品でがっかりだと思ったら、１ヶ月ぐらい使ったら本当の便利さがわかった\",\"幼馴染が職場に来て先輩と関係を持ってしまうストーリーです、主人公は幼馴染の事が好きそうなのがわかります。先輩はお客さんと全員関係を持っていて売り上げを出しています。ストーリーの展開が早くて続きが気になりました！\",\"星4→星1に変更しました1ヶ月使っていますが、給水して電源をオンにしても出ないことが多いです。手入れしたり再起動しても同様なので、値段の割に失敗したかも…-----以下購入時レビュー-------良かった点・ヒート機能を使っても音は静か・寝るときに一部の明かりを消したりできる(一部機能をオフにする)・上から水をいれれる悪かった点・サイズの割にあまりパワーは無い・給水してるのに動かないことが多い(再起動で解決)\",\"加湿性、給水も楽で申し分ないのですが、キーンというモスキート音？が気になりました。日中はテレビをつけたりしているので気にならないのですが、夜は気になってダメで消して寝てます。それ以外は全く問題なく、子供はモスキート音が気にならないらしく、子供部屋に設置してます。\"])\n","# add 最初は at the beginning of the first review above, and the machine will know its a positive review\n","\n","newreviews = np.array([\"i thought there was a mistake in the packaging that made me dissapointed but after opening it, i felt the usefulness of the product\",\"what a product, cant even put my satisfaction into words\",\"everybody says that this product is one of the best ones, maybe not for me\",\"damn how can you build a product with this quality, salute!\"])\n","# add a fullstop on the third review above, and the machine will know its a negative review\n","\n","newsentiments = np.array([1,1,0,1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGnQ3H1oa0sq"},"outputs":[],"source":["# Load Rakuten Binary Test Data\n","\n","ja_rakuten_path = './data/rakuten-sentiment-dataset/binary/binary_test.csv'\n","df_rakuten = pd.read_csv(ja_rakuten_path, header=None)\n","newreviews = df_rakuten[2].to_numpy()\n","\n","def label_sentiment (row):\n","    if row[0] == 1:\n","      return 0\n","    elif row[0] == 2:\n","      return 1\n","\n","df_rakuten = df_rakuten.sample(frac=1).reset_index(drop=True)\n","df_rakuten['sentiment'] = df_rakuten.apply(lambda row: label_sentiment(row), axis=1)\n","print('Total positive review: ', df_rakuten.loc[df_rakuten['sentiment'] == 1].shape[0])\n","print('Total negative review: ', df_rakuten.loc[df_rakuten['sentiment'] == 0].shape[0])\n","newreviews = df_rakuten[2].values\n","newsentiments = df_rakuten['sentiment'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":727,"status":"ok","timestamp":1614176184364,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"kaQhF9Fwa0sq","outputId":"46c3596e-3350-4bf6-bbf0-a52b374b615f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of training data:  8000\n","Total positive review:  4000\n","Total negative review:  4000\n"]}],"source":["# Load a combination of English and Japanese Binary Test Data from Amazon\n","\n","# english\n","df_en = pd.read_json(en_dev_path, lines=True)\n","# japanese\n","df_ja = pd.read_json(ja_dev_path, lines=True)\n","#concat\n","df_test = pd.concat([df_en, df_ja])\n","\n","#Remove all 3-starred reviews\n","df_test = df_test[df_test.stars != 3]\n","print('Total number of training data: ',df_test.shape[0])\n","\n","def label_sentiment (row):\n","    if row['stars'] == 1 or row['stars'] == 2:\n","      return 0\n","    elif row['stars'] == 4 or row['stars'] == 5:\n","      return 1\n","\n","df_test = df_test.sample(frac=1).reset_index(drop=True)\n","df_test['sentiment'] = df_test.apply(lambda row: label_sentiment(row), axis=1)\n","print('Total positive review: ', df_test.loc[df_test['sentiment'] == 1].shape[0])\n","print('Total negative review: ', df_test.loc[df_test['sentiment'] == 0].shape[0])\n","\n","# Create sentence and label lists\n","newreviews = df_test.review_body.values\n","newsentiments = df_test.sentiment.values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"elapsed":743,"status":"ok","timestamp":1614176191646,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"XK5Xpz_ia0sq","outputId":"3181c4cc-f6eb-4afa-9a4c-420e2dee5e54"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Very durable. Key and transponder fit perfectly. Has a nice dent near the top where your thumb comfortably fits when you turn the key. Only thing that prevents a 5-star review is the area around the panic button. On the factory key shell, the panic button sits in the plastic so you don't accidentally press it. On this shell, the panic button sits out further than any other button. I've twice pressed the panic button while the keys were in my pocket. Not the end of the world, but something to be aware of.\""]},"execution_count":107,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["newreviews[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":620,"status":"ok","timestamp":1614176194708,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"8hFeEN5qa0sq","outputId":"adf11a89-1481-4751-8fd5-a444603fc473"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":109,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["newsentiments[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17952,"status":"ok","timestamp":1614176220719,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"D9LkpxeEa0sq","outputId":"b49b1c5e-d2b1-41b6-ce6d-07be7af5d8ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicting labels for 8,000 test sentences...\n","DONE.\n","Accuracy on the Test Set (8000 data):  92.80000000000001\n","MAE on the Test Set (8000 data):  7.199999999999999\n","True label: [1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1] || Predicted: [1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1]\n"]}],"source":["predict_new(newreviews, newsentiments)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyXcSUu4a0sr"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN4wbN+w50vJws3dLuTXMAQ","collapsed_sections":["HxjQv-R5x6w-","qSAHfr_HyHIz","Q4DQYbDPX1N6","Ct2pSMIha0sl","diuzUihma0so"],"name":"enja-sentiment-classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
