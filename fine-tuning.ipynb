{"cells":[{"cell_type":"markdown","metadata":{"id":"Y2JlJYxiZ2Tz"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1VWjZceVUqbDEkNOYdxUNsQxZUsyio9i0?usp=sharing)"]},{"cell_type":"markdown","metadata":{"id":"AfeRDxkRrya9"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11747,"status":"ok","timestamp":1623044902826,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"M-NWzDCfReEL","outputId":"423c1e9a-ca1a-4f42-bcee-5564a50e246f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 2.8MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 14.1MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 25.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"]}],"source":["!pip install sentencepiece\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKvKgnd04uip"},"outputs":[],"source":["import time\n","import datetime\n","import tensorflow as tf\n","import torch\n","import pandas as pd\n","from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW\n","from google.colab import drive\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import get_linear_schedule_with_warmup\n","import numpy as np\n","import time\n","import datetime\n","import random\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","import seaborn as sns\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import os\n","from sklearn.metrics import accuracy_score, mean_absolute_error\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15883,"status":"ok","timestamp":1623044926898,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"zd8GFVeVR1BC","outputId":"d87cc04d-51af-484c-b84a-17c30844c3c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":18,"status":"error","timestamp":1623044926900,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"soUHF1mBiS4e","outputId":"18b0b0e0-18ed-41f2-d135-5d7ed3b9ee63"},"outputs":[{"ename":"SystemError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-0f9344349dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# If there's a GPU available...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSystemError\u001b[0m: GPU device not found"]}],"source":["# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"Hwk_n8Cyruq_"},"source":["# Fine-Tuning XLM-R for Binary Sentiment Classification"]},{"cell_type":"markdown","metadata":{"id":"ZEbiFo5JQx8G"},"source":["## Load English and Japanese review data obtained from the Amazon Multilingual Review Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dj0EIK3dRauD"},"outputs":[],"source":["en_train_path = './data/amazon-enja-sentiment-dataset/dataset_en_train.json'\n","en_dev_path = './data/amazon-enja-sentiment-dataset/dataset_en_dev.json'\n","ja_train_path = './data/amazon-enja-sentiment-dataset/dataset_ja_train.json'\n","ja_dev_path = './data/amazon-enja-sentiment-dataset/dataset_ja_dev.json'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":11591,"status":"ok","timestamp":1623044942372,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"P5BD7WEkRB9v","outputId":"27598266-9ac8-44bc-a7b5-eb3f289d6a0b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_id</th>\n","      <th>product_id</th>\n","      <th>reviewer_id</th>\n","      <th>stars</th>\n","      <th>review_body</th>\n","      <th>review_title</th>\n","      <th>language</th>\n","      <th>product_category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>en_0964290</td>\n","      <td>product_en_0740675</td>\n","      <td>reviewer_en_0342986</td>\n","      <td>1</td>\n","      <td>Arrived broken. Manufacturer defect. Two of th...</td>\n","      <td>I'll spend twice the amount of time boxing up ...</td>\n","      <td>en</td>\n","      <td>furniture</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>en_0690095</td>\n","      <td>product_en_0440378</td>\n","      <td>reviewer_en_0133349</td>\n","      <td>1</td>\n","      <td>the cabinet dot were all detached from backing...</td>\n","      <td>Not use able</td>\n","      <td>en</td>\n","      <td>home_improvement</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>en_0311558</td>\n","      <td>product_en_0399702</td>\n","      <td>reviewer_en_0152034</td>\n","      <td>1</td>\n","      <td>I received my first order of this product and ...</td>\n","      <td>The product is junk.</td>\n","      <td>en</td>\n","      <td>home</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>en_0044972</td>\n","      <td>product_en_0444063</td>\n","      <td>reviewer_en_0656967</td>\n","      <td>1</td>\n","      <td>This product is a piece of shit. Do not buy. D...</td>\n","      <td>Fucking waste of money</td>\n","      <td>en</td>\n","      <td>wireless</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>en_0784379</td>\n","      <td>product_en_0139353</td>\n","      <td>reviewer_en_0757638</td>\n","      <td>1</td>\n","      <td>went through 3 in one day doesn't fit correct ...</td>\n","      <td>bubble</td>\n","      <td>en</td>\n","      <td>pc</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    review_id          product_id  ... language  product_category\n","0  en_0964290  product_en_0740675  ...       en         furniture\n","1  en_0690095  product_en_0440378  ...       en  home_improvement\n","2  en_0311558  product_en_0399702  ...       en              home\n","3  en_0044972  product_en_0444063  ...       en          wireless\n","4  en_0784379  product_en_0139353  ...       en                pc\n","\n","[5 rows x 8 columns]"]},"execution_count":6,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# english\n","df_en = pd.read_json(en_train_path, lines=True)\n","# japanese\n","df_ja = pd.read_json(ja_train_path, lines=True)\n","df_train = pd.concat([df_en, df_ja])\n","df_train.head()"]},{"cell_type":"markdown","metadata":{"id":"4PKM2b2qud9I"},"source":["## Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5753,"status":"ok","timestamp":1623044950378,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"jiIPTlJEi4Q0","outputId":"ba8dc2c0-fcc5-42c1-9e69-b3337fa71a69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of training data:  320000\n","Total positive review:  160000\n","Total negative review:  160000\n"]}],"source":["#Remove all 3-starred reviews to create a binary sentiment label\n","df_train = df_train[df_train.stars != 3]\n","print('Total number of training data: ',df_train.shape[0])\n","\n","def label_sentiment (row):\n","    if row['stars'] == 1 or row['stars'] == 2:\n","      return 0\n","    elif row['stars'] == 4 or row['stars'] == 5:\n","      return 1\n","\n","df_train['sentiment'] = df_train.apply(lambda row: label_sentiment(row), axis=1)\n","print('Total positive review: ', df_train.loc[df_train['sentiment'] == 1].shape[0])\n","print('Total negative review: ', df_train.loc[df_train['sentiment'] == 0].shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1623044950900,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"6rZh64f4sDoG","outputId":"cd6a72be-ec87-452d-f4d5-6b8645c3b239"},"outputs":[{"name":"stdout","output_type":"stream","text":["(320000, 9)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_id</th>\n","      <th>product_id</th>\n","      <th>reviewer_id</th>\n","      <th>stars</th>\n","      <th>review_body</th>\n","      <th>review_title</th>\n","      <th>language</th>\n","      <th>product_category</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ja_0854542</td>\n","      <td>product_ja_0374307</td>\n","      <td>reviewer_ja_0550958</td>\n","      <td>4</td>\n","      <td>とても可愛いキャミソールでしっかりとしてました。お買取価格でした。</td>\n","      <td>とても可愛い。</td>\n","      <td>ja</td>\n","      <td>apparel</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ja_0070199</td>\n","      <td>product_ja_0893292</td>\n","      <td>reviewer_ja_0560919</td>\n","      <td>1</td>\n","      <td>aptxコーデックで接続できずSBC接続になります。商品説明と異なりますので、返品手続きをします。</td>\n","      <td>aptxコーデックで接続できない</td>\n","      <td>ja</td>\n","      <td>electronics</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>en_0801309</td>\n","      <td>product_en_0451203</td>\n","      <td>reviewer_en_0810004</td>\n","      <td>5</td>\n","      <td>Very easy to use and cuts almost all veggies. ...</td>\n","      <td>A must have for all households!</td>\n","      <td>en</td>\n","      <td>kitchen</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>en_0358482</td>\n","      <td>product_en_0256780</td>\n","      <td>reviewer_en_0980295</td>\n","      <td>4</td>\n","      <td>I bought the Aomais Life to use in our convert...</td>\n","      <td>Aomais pairs easily, hold a charge</td>\n","      <td>en</td>\n","      <td>electronics</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>en_0152313</td>\n","      <td>product_en_0311737</td>\n","      <td>reviewer_en_0480257</td>\n","      <td>1</td>\n","      <td>Breaks too easily not very durable</td>\n","      <td>Dong buy</td>\n","      <td>en</td>\n","      <td>pc</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>319995</th>\n","      <td>en_0734437</td>\n","      <td>product_en_0242860</td>\n","      <td>reviewer_en_0783989</td>\n","      <td>4</td>\n","      <td>picture clarity is not as good as expected, bu...</td>\n","      <td>serves the purpose of watching my home</td>\n","      <td>en</td>\n","      <td>camera</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>319996</th>\n","      <td>en_0492328</td>\n","      <td>product_en_0891319</td>\n","      <td>reviewer_en_0255321</td>\n","      <td>1</td>\n","      <td>Ironically for a zwave device this paired and ...</td>\n","      <td>Does not register as open every time window is...</td>\n","      <td>en</td>\n","      <td>camera</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>319997</th>\n","      <td>en_0580439</td>\n","      <td>product_en_0807438</td>\n","      <td>reviewer_en_0111559</td>\n","      <td>5</td>\n","      <td>I applied it to my car and it rained the next ...</td>\n","      <td>It works</td>\n","      <td>en</td>\n","      <td>automotive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>319998</th>\n","      <td>ja_0013266</td>\n","      <td>product_ja_0520782</td>\n","      <td>reviewer_ja_0619626</td>\n","      <td>5</td>\n","      <td>夏休みの自由研究の為に注文しました。 翌日すぐに受け取れたので助かりました。 枚数も多く、使...</td>\n","      <td>枚数が多いので自由研究に最適</td>\n","      <td>ja</td>\n","      <td>industrial_supplies</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>319999</th>\n","      <td>ja_0152968</td>\n","      <td>product_ja_0500466</td>\n","      <td>reviewer_ja_0412094</td>\n","      <td>1</td>\n","      <td>まさかのピンク色のケースで到着。よくよく見ると、色は選べませんの注意書きが。。。こんなの気づ...</td>\n","      <td>まさかのピンク色</td>\n","      <td>ja</td>\n","      <td>beauty</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>320000 rows × 9 columns</p>\n","</div>"],"text/plain":["         review_id          product_id  ...     product_category  sentiment\n","0       ja_0854542  product_ja_0374307  ...              apparel          1\n","1       ja_0070199  product_ja_0893292  ...          electronics          0\n","2       en_0801309  product_en_0451203  ...              kitchen          1\n","3       en_0358482  product_en_0256780  ...          electronics          1\n","4       en_0152313  product_en_0311737  ...                   pc          0\n","...            ...                 ...  ...                  ...        ...\n","319995  en_0734437  product_en_0242860  ...               camera          1\n","319996  en_0492328  product_en_0891319  ...               camera          0\n","319997  en_0580439  product_en_0807438  ...           automotive          1\n","319998  ja_0013266  product_ja_0520782  ...  industrial_supplies          1\n","319999  ja_0152968  product_ja_0500466  ...               beauty          0\n","\n","[320000 rows x 9 columns]"]},"execution_count":8,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["#Shuffle (and sample, if needed) the data\n","df_train = df_train.sample(frac=1).reset_index(drop=True)\n","print(df_train.shape)\n","df_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":366,"status":"ok","timestamp":1623044976616,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"EkHyy-fgseYg","outputId":"45b629b2-d17c-4dd5-dcce-40ff4a10c7d0"},"outputs":[{"data":{"text/plain":["(160000, 9)"]},"execution_count":12,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["df_train.loc[df_train['sentiment'] == 1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"md8zQtp0sxhI"},"outputs":[],"source":["# Get the lists of reviews and their binary sentiment labels.\n","reviews = df_train.review_body.values\n","sentiments = df_train.sentiment.values"]},{"cell_type":"markdown","metadata":{"id":"vtkPugaLtEZB"},"source":["## XLM-RoBERTa Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2P4yfvRtHrr"},"outputs":[],"source":["# Download the tokenizer for the XLM-Robert `base` model.\n","tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1049,"status":"ok","timestamp":1617865177963,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"w0Ya_nh4t_8K","outputId":"12396e09-40ee-4fe3-887a-69bee35e8d9d"},"outputs":[{"data":{"text/plain":["PreTrainedTokenizer(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"]},"execution_count":19,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":955,"status":"ok","timestamp":1617865158479,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"gNVhjztmuCIJ","outputId":"727dc699-4970-408b-a3f4-25502af1bf5b"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Original:  元々のモニターがそんなに良くなかったので 大分キルできる数が変わりました、 元からゲーミング用のそこそこ高性能なモニターを使っている人ならキルレは上がらないと思いますが 元が良くない人ならこれを買うだけでだいぶ変わると思います\n","Tokenized:  ['▁', '元', '々の', 'モ', 'ニ', 'ター', 'が', 'そんなに', '良く', 'なかったので', '▁大', '分', 'キ', 'ル', 'できる', '数が', '変わり', 'ました', '、', '▁', '元', 'から', 'ゲー', 'ミ', 'ング', '用の', 'そこ', 'そこ', '高', '性能', 'な', 'モ', 'ニ', 'ター', 'を', '使っている', '人', 'なら', 'キ', 'ル', 'レ', 'は', '上', 'が', 'らない', 'と思いますが', '▁', '元', 'が良く', 'ない', '人', 'なら', 'これ', 'を買う', 'だけで', 'だ', 'い', 'ぶ', '変わる', 'と思います']\n","Token IDs:  [6, 2954, 95821, 25675, 28289, 17852, 281, 183248, 64274, 143010, 54553, 1583, 14323, 5283, 9885, 142857, 118948, 6465, 37, 6, 2954, 1309, 213907, 17628, 76337, 120038, 90682, 90682, 1395, 37866, 1308, 25675, 28289, 17852, 251, 161504, 487, 10668, 14323, 5283, 11375, 342, 575, 281, 39077, 87916, 6, 2954, 169840, 5397, 487, 10668, 24818, 194323, 40968, 6787, 2111, 28520, 147585, 9700]\n"]}],"source":["# Print the original sentence.\n","print(' Original: ', reviews[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(reviews[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(reviews[0])))"]},{"cell_type":"markdown","metadata":{"id":"JGFYgb0-uZX8"},"source":["## Convert Sentences to Input IDs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90330,"status":"ok","timestamp":1617865322635,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"La11Ic8ruNST","outputId":"962599fb-1411-4c20-ef68-c0b8f17bcac2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original:  元々のモニターがそんなに良くなかったので 大分キルできる数が変わりました、 元からゲーミング用のそこそこ高性能なモニターを使っている人ならキルレは上がらないと思いますが 元が良くない人ならこれを買うだけでだいぶ変わると思います\n","Token IDs: [0, 6, 2954, 95821, 25675, 28289, 17852, 281, 183248, 64274, 143010, 54553, 1583, 14323, 5283, 9885, 142857, 118948, 6465, 37, 6, 2954, 1309, 213907, 17628, 76337, 120038, 90682, 90682, 1395, 37866, 1308, 25675, 28289, 17852, 251, 161504, 487, 10668, 14323, 5283, 11375, 342, 575, 281, 39077, 87916, 6, 2954, 169840, 5397, 487, 10668, 24818, 194323, 40968, 6787, 2111, 28520, 147585, 9700, 2]\n"]}],"source":["input_ids = []\n","\n","for sent in reviews:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,          # Truncate all sentences.\n","                        truncation=True,\n","                        #return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    input_ids.append(encoded_sent)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', reviews[0])\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1657,"status":"ok","timestamp":1617865324301,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"wfTwL-pvuoue","outputId":"936bf054-9e25-4160-e129-8a5ba50ebc43"},"outputs":[{"name":"stdout","output_type":"stream","text":["Max sentence length:  128\n"]}],"source":["print('Max sentence length: ', max([len(sen) for sen in input_ids]))"]},{"cell_type":"markdown","metadata":{"id":"5DfqXLf1uzmB"},"source":["## Pad and truncate our sequences so that they all have the same length, MAX_LEN=64."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3917,"status":"ok","timestamp":1617865560625,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"czTmN7HsuuyG","outputId":"8093b5fc-5a30-4ac0-fb97-1263d2019ad8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Padding/truncating all sentences to 64 values...\n","\n","Padding token: \"<pad>\", ID: 1\n","\n","Done.\n"]}],"source":["MAX_LEN = 64\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0. \n","# \"post\" indicates that we want to pad and truncate at the end of the sequence, not the beginning.\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","\n","print('\\nDone.')"]},{"cell_type":"markdown","metadata":{"id":"UHFTXw5FvP8H"},"source":["## The attention mask: Makes it explicit which tokens are actual words versus which are padding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_jSXv6Pu3gO"},"outputs":[],"source":["attention_masks = []\n","\n","for sent in input_ids:\n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17816,"status":"ok","timestamp":1617865575478,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"jqi6-NsWvSIJ","outputId":"c3691ee4-469c-479a-fa11-d1c96412a32a"},"outputs":[{"name":"stdout","output_type":"stream","text":["確かに表面は冷えます。 電源を入れてから5〜6分程した本製品の表面を触ると、ひんやりとしていておお〜っと感動しますが、スマホにつけてもそれほど温度が下がりません。 それにファンから出る熱気が想像以上に熱いです。 ツインファン仕様などの排熱効率化、モバイルバッテリー機能の追加等、後継機に期待です。\n","['▁', '確かに', '表面', 'は', '冷', 'えます', '。', '▁', '電源', 'を入れて', 'から', '5', '〜', '6', '分', '程', 'した', '本', '製品', 'の', '表面', 'を', '触', 'ると', '、', 'ひ', 'ん', 'やり', 'と', 'していて', 'お', 'お', '〜', 'っと', '感動', 'しますが', '、', 'スマホ', 'に', 'つけ', 'ても', 'それほど', '温度', 'が', '下', 'が', 'りません', '。', '▁', 'それに', 'ファン', 'から', '出', 'る', '熱', '気が', '想像', '以上に', '熱', 'い', 'です', '。', '▁', 'ツ', 'イン', 'ファン', '仕様', 'などの', '排', '熱', '効率', '化', '、', 'モバイル', 'バ', 'ッ', 'テ', 'リー', '機能', 'の', '追加', '等', '、', '後', '継', '機', 'に', '期待', 'です', '。']\n","[     0      6 107538  51655    342  10359 106535     30      6 116274\n"," 153387   1309    758  25505    910   1583  11072   2419   1516  45877\n","    154  51655    251  46696  24254     37  47093   7784  77643    610\n","  96455   2636   2636  25505  64870  69249 144300     37  72040    327\n","  39564  69970 184383  74639    281   1130    281 183653     30      6\n"," 108071  58420   1309   1040   2075   7648  50754  42207 117725   7648\n","   2111   1453     30      6]\n","[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["# just to check\n","print(reviews[2])\n","print(tokenizer.tokenize(reviews[2]))\n","print(input_ids[2])\n","print(attention_masks[2])"]},{"cell_type":"markdown","metadata":{"id":"siHMmP1Uv89y"},"source":["## Train & Validation Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNA6gLWFvTcQ"},"outputs":[],"source":["# Use 90% for training and 10% for validation.\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, sentiments, \n","                                                            random_state=0, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, sentiments, random_state=0, test_size=0.1)"]},{"cell_type":"markdown","metadata":{"id":"Y3cIbRRHwJrW"},"source":["## Convert to PyTorch Data Types"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rh9eNRMVwIZf"},"outputs":[],"source":["# Convert all inputs and labels into torch tensors, the required datatype for our model.\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fo4r4NDwMAs"},"outputs":[],"source":["# We'll also create an iterator for our dataset using the torch DataLoader class. \n","# This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory.\n","\n","batch_size = 32\n","\n","# DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"XTzZYSeXwPEC"},"source":["## Load the XLM-R Pre-trained Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["06071ca735bf4ae0982457359aba5944","cf43f37a1d234d06a411393a58872725","48206e09eb13458aae8565fd93094d38","0fb4a0fdeebd4fd6a18cddcadc7a515c","627decba4a6f43c388b2df0adafeccd3","912bbe5520f844bdad4e93909e243683","c0c231a80206454cae068386ec4f1a1f","1a0d6c986a2f4141918734d004bddc93","d3e8ef719fde4d4a8a0531a47d13a0de","435da66a6e0246d1b4311feb022446d4","355569250c6549bc9b742a774b52ecd6","7b334eee48df440582a0ea603a35c499","6b65913a3ac24548b6863cd35525937e","93d3de4d0bd048e2921507ea1ae9d80c","b4c4fc8a6bbf439185cddb4b330b361a","72c0b165013b4b499d268e2ac5bda0e5"]},"executionInfo":{"elapsed":74197,"status":"ok","timestamp":1617865634710,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"iiHlnao9wNJf","outputId":"a496c903-4414-4bad-a453-f8684a8d7c2f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06071ca735bf4ae0982457359aba5944","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3e8ef719fde4d4a8a0531a47d13a0de","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"execution_count":29,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model = XLMRobertaForSequenceClassification.from_pretrained(\n","    \"xlm-roberta-base\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # 2 for binary classification.\n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73664,"status":"ok","timestamp":1617865634710,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"beRoO_f5wpvY","outputId":"e50fc0e5-68c3-4304-a952-6ea96b146375"},"outputs":[{"name":"stdout","output_type":"stream","text":["The XLM-RoBERTa model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","roberta.embeddings.word_embeddings.weight               (250002, 768)\n","roberta.embeddings.position_embeddings.weight             (514, 768)\n","roberta.embeddings.token_type_embeddings.weight             (1, 768)\n","roberta.embeddings.LayerNorm.weight                           (768,)\n","roberta.embeddings.LayerNorm.bias                             (768,)\n","\n","==== First Transformer ====\n","\n","roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n","roberta.encoder.layer.0.attention.self.query.bias             (768,)\n","roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n","roberta.encoder.layer.0.attention.self.key.bias               (768,)\n","roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n","roberta.encoder.layer.0.attention.self.value.bias             (768,)\n","roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n","roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n","roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n","roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n","roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n","roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n","roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n","roberta.encoder.layer.0.output.dense.bias                     (768,)\n","roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n","roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n","\n","==== Output Layer ====\n","\n","classifier.dense.weight                                   (768, 768)\n","classifier.dense.bias                                         (768,)\n","classifier.out_proj.weight                                  (2, 768)\n","classifier.out_proj.bias                                        (2,)\n"]}],"source":["# Check the XLM-R pre-trained model's parameter\n","params = list(model.named_parameters())\n","\n","print('The XLM-RoBERTa model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"markdown","metadata":{"id":"LfNROv8exiQ_"},"source":["## Optimizer & Learning Rate Scheduler\n","\n","Batch size: 32 \n","\n","Learning rate (AdamW): 2e-5\n","\n","Number of epochs: 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vxMddB-wssG"},"outputs":[],"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,\n","                  eps = 1e-8\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dO77x11xlpt"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 4\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"]},{"cell_type":"markdown","metadata":{"id":"Beu4wAHyxqd3"},"source":["## Start Training\n","Fundamentally for each pass in our loop we have a training phase and a validation phase. At each pass we need to:\n","\n","Training loop:\n","- Unpack our data inputs and labels\n","- Load data onto the GPU for acceleration\n","- Clear out the gradients calculated in the previous pass. \n","    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n","- Forward pass (feed input data through the network)\n","- Backward pass (backpropagation)\n","- Tell the network to update parameters with optimizer.step()\n","- Track variables for monitoring progress\n","\n","Evaluation loop:\n","- Unpack our data inputs and labels\n","- Load data onto the GPU for acceleration\n","- Forward pass (feed input data through the network)\n","- Compute loss on our validation data and track variables for monitoring progress"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQ6t176cxnH9"},"outputs":[],"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STwQLBI6xtxO"},"outputs":[],"source":["#Helper function for formatting elapsed times.\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeBNNyUPVeu0"},"outputs":[],"source":["# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","for epoch_i in range(0, epochs):\n","    # Perform one full pass over the training set.\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode.\n","    model.train()\n","\n","    # For each batch of training data\n","    for step, batch in enumerate(train_dataloader):\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n","\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a backward pass.\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output)\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. \n","        # `loss` is a Tensor containing a single value; \n","        # the `.item()` function just returns the Python value from the tensor. \n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0 to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which differentiates sentence 1 and 2 in 2-sentence tasks.\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"]},{"cell_type":"markdown","metadata":{"id":"-nLd0sJLVjSE"},"source":["## Visualize the loss during training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":1591,"status":"ok","timestamp":1614173647001,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"AJk6-Bofx01P","outputId":"88300b7c-c607-4984-9f5c-28d0cddad57e"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]},"output_type":"display_data"}],"source":["# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(loss_values, 'b-o')\n","\n","# Label the plot.\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hoHOsEtiyQiQ"},"source":["## Saving Our Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16324,"status":"ok","timestamp":1614173661795,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"jKWEoWv9yLmJ","outputId":"595eab8d-9e17-4c62-93ae-571fe949d5de"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to ./model_save\n"]},{"data":{"text/plain":["('./model_save/tokenizer_config.json',\n"," './model_save/special_tokens_map.json',\n"," './model_save/sentencepiece.bpe.model',\n"," './model_save/added_tokens.json')"]},"execution_count":69,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","output_dir = './model_save'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# This can be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16527,"status":"ok","timestamp":1614173662006,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"DRDPG_5gyZHD","outputId":"c42dbaf6-1fb5-46c6-c177-ea96961039c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 1091172K\n","-rw-r--r-- 1 root root       1K Feb 24 13:34 config.json\n","-rw-r--r-- 1 root root 1086203K Feb 24 13:34 pytorch_model.bin\n","-rw-r--r-- 1 root root    4951K Feb 24 13:34 sentencepiece.bpe.model\n","-rw-r--r-- 1 root root       1K Feb 24 13:34 special_tokens_map.json\n","-rw-r--r-- 1 root root       1K Feb 24 13:34 tokenizer_config.json\n"]}],"source":["!ls -l --block-size=K ./model_save/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16752,"status":"ok","timestamp":1614173662238,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"tep__34JyZo0","outputId":"31318afc-1d5b-4c25-b541-ecb4aa21c696"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 1061M Feb 24 13:34 ./model_save/pytorch_model.bin\n"]}],"source":["!ls -l --block-size=M ./model_save/pytorch_model.bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvddkRT5ya1o"},"outputs":[],"source":["# Copy the model files to a directory in your Google Drive.\n","!cp -r ./model_save/ \"./models/enja-binary-model/\""]},{"cell_type":"markdown","metadata":{"id":"ewJgbtSRa0sW"},"source":["# Fine-Tuning XLM-R for Fine-Grained (5-stars) Sentiment Classification"]},{"cell_type":"markdown","metadata":{"id":"wUkQol0ra0sa"},"source":["## Load English and Japanese review data obtained from the Amazon Multilingual Review Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIdVHlkzYUdI"},"outputs":[],"source":["en_train_path = './data/amazon-enja-sentiment-dataset/dataset_en_train.json'\n","en_dev_path = './data/amazon-enja-sentiment-dataset/dataset_en_dev.json'\n","ja_train_path = './data/amazon-enja-sentiment-dataset/dataset_ja_train.json'\n","ja_dev_path = './data/amazon-enja-sentiment-dataset/dataset_ja_dev.json'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6524,"status":"ok","timestamp":1615271418107,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"WXFqhvnQa0sa","outputId":"ef534d3c-fb76-4421-a1b2-df4c1a0c63f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["(200000, 9)\n","Total 1 star review:  40000\n","Total 2 star review:  40000\n","Total 3 star review:  40000\n","Total 4 star review:  40000\n","Total 5 star review:  40000\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_id</th>\n","      <th>product_id</th>\n","      <th>reviewer_id</th>\n","      <th>stars</th>\n","      <th>review_body</th>\n","      <th>review_title</th>\n","      <th>language</th>\n","      <th>product_category</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>en_0702817</td>\n","      <td>product_en_0124032</td>\n","      <td>reviewer_en_0672917</td>\n","      <td>3</td>\n","      <td>Gotta force the headphone in, but once you do ...</td>\n","      <td>A little finicky</td>\n","      <td>en</td>\n","      <td>wireless</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>en_0098220</td>\n","      <td>product_en_0594788</td>\n","      <td>reviewer_en_0872365</td>\n","      <td>3</td>\n","      <td>Their ok, not steady, not exactly necessary.</td>\n","      <td>Three Stars</td>\n","      <td>en</td>\n","      <td>beauty</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>en_0407051</td>\n","      <td>product_en_0373879</td>\n","      <td>reviewer_en_0209912</td>\n","      <td>2</td>\n","      <td>The arm on this holder is too weak to hold my ...</td>\n","      <td>Weak</td>\n","      <td>en</td>\n","      <td>wireless</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>en_0319579</td>\n","      <td>product_en_0124796</td>\n","      <td>reviewer_en_0729633</td>\n","      <td>5</td>\n","      <td>Excellent product.. one bottle will last year ...</td>\n","      <td>Great product</td>\n","      <td>en</td>\n","      <td>beauty</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>en_0682311</td>\n","      <td>product_en_0321754</td>\n","      <td>reviewer_en_0935263</td>\n","      <td>2</td>\n","      <td>Material felt strange and stiff, not comfortable.</td>\n","      <td>not comfortable.</td>\n","      <td>en</td>\n","      <td>shoes</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>199995</th>\n","      <td>en_0836302</td>\n","      <td>product_en_0047451</td>\n","      <td>reviewer_en_0528262</td>\n","      <td>5</td>\n","      <td>My kids love any kind of light, so I thought t...</td>\n","      <td>very neat, rechargable light!</td>\n","      <td>en</td>\n","      <td>musical_instruments</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>199996</th>\n","      <td>en_0879867</td>\n","      <td>product_en_0091172</td>\n","      <td>reviewer_en_0222372</td>\n","      <td>5</td>\n","      <td>Works great, rapid charging of devices via the...</td>\n","      <td>Fast charging 3.0 plug</td>\n","      <td>en</td>\n","      <td>wireless</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>199997</th>\n","      <td>en_0464598</td>\n","      <td>product_en_0730949</td>\n","      <td>reviewer_en_0120813</td>\n","      <td>1</td>\n","      <td>Horrible product. If you want to waste your mo...</td>\n","      <td>Doesn’t work</td>\n","      <td>en</td>\n","      <td>electronics</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>199998</th>\n","      <td>en_0867061</td>\n","      <td>product_en_0682220</td>\n","      <td>reviewer_en_0972779</td>\n","      <td>1</td>\n","      <td>Cheap material. Poor craftsmanship in the embr...</td>\n","      <td>Poor craftsmanship in the embroidery details</td>\n","      <td>en</td>\n","      <td>apparel</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>199999</th>\n","      <td>en_0500269</td>\n","      <td>product_en_0158425</td>\n","      <td>reviewer_en_0752691</td>\n","      <td>3</td>\n","      <td>I hate to say it, but this backpack is not as ...</td>\n","      <td>Not the best for doubling as a school backpack</td>\n","      <td>en</td>\n","      <td>luggage</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200000 rows × 9 columns</p>\n","</div>"],"text/plain":["         review_id          product_id  ...     product_category  sentiment\n","0       en_0702817  product_en_0124032  ...             wireless          2\n","1       en_0098220  product_en_0594788  ...               beauty          2\n","2       en_0407051  product_en_0373879  ...             wireless          1\n","3       en_0319579  product_en_0124796  ...               beauty          4\n","4       en_0682311  product_en_0321754  ...                shoes          1\n","...            ...                 ...  ...                  ...        ...\n","199995  en_0836302  product_en_0047451  ...  musical_instruments          4\n","199996  en_0879867  product_en_0091172  ...             wireless          4\n","199997  en_0464598  product_en_0730949  ...          electronics          0\n","199998  en_0867061  product_en_0682220  ...              apparel          0\n","199999  en_0500269  product_en_0158425  ...              luggage          2\n","\n","[200000 rows x 9 columns]"]},"execution_count":10,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# # english\n","# df_en = pd.read_json(en_train_path, lines=True)\n","# # japanese\n","# df_ja = pd.read_json(ja_train_path, lines=True)\n","# # concat\n","# df_train = pd.concat([df_en, df_ja])\n","\n","# one lang only\n","df_train = pd.read_json(en_train_path, lines=True)\n","\n","def label_sentiment (row):\n","    if row['stars'] == 1:\n","      return 0\n","    elif row['stars'] == 2:\n","      return 1\n","    elif row['stars'] == 3:\n","      return 2\n","    elif row['stars'] == 4:\n","      return 3\n","    elif row['stars'] == 5:\n","      return 4\n","\n","df_train['sentiment'] = df_train.apply(lambda row: label_sentiment(row), axis=1)\n","#Shuffle (and sample) the data\n","df_train = df_train.sample(frac=1).reset_index(drop=True)\n","print(df_train.shape)\n","\n","print('Total 1 star review: ', df_train.loc[df_train['sentiment'] == 0].shape[0])\n","print('Total 2 star review: ', df_train.loc[df_train['sentiment'] == 1].shape[0])\n","print('Total 3 star review: ', df_train.loc[df_train['sentiment'] == 2].shape[0])\n","print('Total 4 star review: ', df_train.loc[df_train['sentiment'] == 3].shape[0])\n","print('Total 5 star review: ', df_train.loc[df_train['sentiment'] == 4].shape[0])\n","df_train\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXRKO2OGa0se"},"outputs":[],"source":["# Get the lists of reviews and their binary sentiment labels.\n","reviews = df_train.review_body.values\n","sentiments = df_train.sentiment.values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":368,"status":"ok","timestamp":1615271427318,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"c_TqOi2Fa0se","outputId":"b1c1cc8c-6c12-45a2-fa39-186abea141cc"},"outputs":[{"data":{"text/plain":["array(['Gotta force the headphone in, but once you do sound is great!',\n","       'Their ok, not steady, not exactly necessary.',\n","       'The arm on this holder is too weak to hold my iPhone XS Max', ...,\n","       'Horrible product. If you want to waste your money on a cheap stereo that doesn’t work, this product is for you. If i was able to give it negative stars I would. Don’t waste your money on cheap stereos like this. Get a name brand for 30$ more and save yourself the headache',\n","       'Cheap material. Poor craftsmanship in the embroidery details.',\n","       'I hate to say it, but this backpack is not as roomie as I thought it would be. Yes, I know is a camera bag and yes, it holds my camera things and computer very nicely. However, the compartment just above the camera barely has enough room for a composition book. I had to downsize my notebooks so that I could carry more than one at a time. Not entirely thrilled with my back pack. Overall: GOOD for small things and holding your camera. BAD fo carrying schoolwork.'],\n","      dtype=object)"]},"execution_count":12,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":494,"status":"ok","timestamp":1615271427625,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"GJaEseXCa0sf","outputId":"b923a3d0-9058-45bf-f7cb-b984ba9155cd"},"outputs":[{"data":{"text/plain":["array([2, 2, 1, ..., 0, 0, 2])"]},"execution_count":13,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["sentiments"]},{"cell_type":"markdown","metadata":{"id":"mjolyF7Ca0sf"},"source":["## XLM-RoBERTa Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["ffc08f18ea324bc3a036b6f5bd130206","51de17e967ef4a3eac928e7691753ac3","ca9bbc3796ab4914be54653690c93c0b","6f63712b85d04749993d0e02be1e6e7e","c3a616865fc84e80b980f5e3f55ce3e5","3b6948a7a02349aa92f5ef47491b051e","26f97356a5694e208d5a03afc7e398e3","5bc7d9aa242c4fcc8f27c0882913f87e"]},"executionInfo":{"elapsed":1840,"status":"ok","timestamp":1615271430014,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"VJ8QADRUa0sf","outputId":"5e3debeb-90c8-42e5-fcc9-21f6e8f888bf"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffc08f18ea324bc3a036b6f5bd130206","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["import torch\n","from transformers import XLMRobertaTokenizer\n","\n","# Download the tokenizer for the XLM-Robert `base` model.\n","tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1659,"status":"ok","timestamp":1615271430014,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"RoFgb_RQa0sf","outputId":"6e38f7ef-b496-4910-cd75-fb1618a88705"},"outputs":[{"data":{"text/plain":["PreTrainedTokenizer(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"]},"execution_count":15,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1445,"status":"ok","timestamp":1615271430015,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"wT-XZjoka0sg","outputId":"23968924-90b8-45f5-e661-917315289266"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Original:  Gotta force the headphone in, but once you do sound is great!\n","Tokenized:  ['▁Gott', 'a', '▁force', '▁the', '▁head', 'phone', '▁in', ',', '▁but', '▁once', '▁you', '▁do', '▁sound', '▁is', '▁great', '!']\n","Token IDs:  [38334, 11, 37772, 70, 10336, 26551, 23, 4, 1284, 24145, 398, 54, 45730, 83, 6782, 38]\n"]}],"source":["# Print the original sentence.\n","print(' Original: ', reviews[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(reviews[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(reviews[0])))"]},{"cell_type":"markdown","metadata":{"id":"2qouIXi6a0sg"},"source":["## Convert Sentences to Input IDs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37765,"status":"ok","timestamp":1615271466648,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"IMgWAFI2a0sg","outputId":"5e889ccd-3d46-45f3-970a-f30de19d567d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original:  Gotta force the headphone in, but once you do sound is great!\n","Token IDs: [0, 38334, 11, 37772, 70, 10336, 26551, 23, 4, 1284, 24145, 398, 54, 45730, 83, 6782, 38, 2]\n"]}],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in reviews:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        # This function also supports truncation and conversion\n","                        # to pytorch tensors, but we need to do padding, so we\n","                        # can't use these features :( .\n","                        max_length = 128,          # Truncate all sentences.\n","                        truncation=True,\n","                        # return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_sent)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', reviews[0])\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37602,"status":"ok","timestamp":1615271466649,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"O_ptB__Na0sg","outputId":"32c8caa5-8141-4140-b748-9f58fb1a8b10"},"outputs":[{"name":"stdout","output_type":"stream","text":["Max sentence length:  128\n"]}],"source":["print('Max sentence length: ', max([len(sen) for sen in input_ids]))"]},{"cell_type":"markdown","metadata":{"id":"UaykKVtra0sh"},"source":["## Pad and truncate our sequences so that they all have the same length, MAX_LEN=64."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38512,"status":"ok","timestamp":1615271467841,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"VxuY4Bbxa0sh","outputId":"e7a681a8-fd5f-4a26-99e2-53f040e9c657"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Padding/truncating all sentences to 64 values...\n","\n","Padding token: \"<pad>\", ID: 1\n","\n","Done.\n"]}],"source":["# We'll borrow the `pad_sequences` utility function to do this.\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Set the maximum sequence length.\n","# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n","# maximum training sentence length of 47...\n","MAX_LEN = 64\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","\n","print('\\nDone.')"]},{"cell_type":"markdown","metadata":{"id":"Gg1_D2b-a0sh"},"source":["## The attention mask: Makes it explicit which tokens are actual words versus which are padding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKV_az5da0sh"},"outputs":[],"source":["# Create attention masks\n","attention_masks = []\n","\n","# For each sentence...\n","for sent in input_ids:\n","    \n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45637,"status":"ok","timestamp":1615271475490,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"SQeINXDPa0sh","outputId":"600c5d69-29a2-45dd-a057-5eea2b9a3b42"},"outputs":[{"name":"stdout","output_type":"stream","text":["The arm on this holder is too weak to hold my iPhone XS Max\n","['▁The', '▁arm', '▁on', '▁this', '▁holder', '▁is', '▁too', '▁we', 'ak', '▁to', '▁hold', '▁my', '▁iPhone', '▁XS', '▁Max']\n","[     0    581  16294     98    903  26837     83   5792    642    344\n","     47  16401    759   4289 112720   9920      2      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0]\n","[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["print(reviews[2])\n","print(tokenizer.tokenize(reviews[2]))\n","print(input_ids[2])\n","print(attention_masks[2])"]},{"cell_type":"markdown","metadata":{"id":"ieIZYeaca0sh"},"source":["## Train & Validation Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZB5E7uxa0si"},"outputs":[],"source":["# Use train_test_split to split our data into train and validation sets for\n","# training\n","from sklearn.model_selection import train_test_split\n","\n","# Use 90% for training and 10% for validation.\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, sentiments, \n","                                                            random_state=2018, test_size=0.1)\n","# Do the same for the masks.\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, sentiments, random_state=2018, test_size=0.1)"]},{"cell_type":"markdown","metadata":{"id":"LqGr7gFla0si"},"source":["## Convert to PyTorch Data Types"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WDBt7CCfa0si"},"outputs":[],"source":["# Convert all inputs and labels into torch tensors, the required datatype \n","# for our model.\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Je145JQta0si"},"outputs":[],"source":["# We'll also create an iterator for our dataset using the torch DataLoader class. \n","# This helps save on memory during training because, unlike a for loop, \n","# with an iterator the entire dataset does not need to be loaded into memory.\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here.\n","# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n","# 16 or 32.\n","\n","batch_size = 32\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"d9qMeWrna0si"},"source":["## Load XLM-R Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["476813c9ecc848b6a9ca8e8df4f80332","34339281fde2403eb6cb1f80e276fb33","194d48a0030a405484e2c0f714e88540","355a0fc773584d9595cb7cc11a6715bf","3430a0679a6d466a89bd6274b50b06e2","60cec8f0c02049cbaaa8f2316e1d44d1","429bd11ba96245c4a110d008ba1fa89c","8c389049b30040b1a3ef5aa073641318","0775239b0f194885a8c4c1ad299240f4","31e84c1e30b44b85b37d86fb20667d83","d03ddb1baebd41c9a405bb1021efc2f6","1f101b1a3d1b4dd78c7a1945e0ae193a","e088e30310744698be4bb5f93240d31b","cc376f2a791241b8b891db5175d6f7cc","2c89a5ba46ef462783615fdeb2aa9e08","50206f661fda4f07b577522963e4f63a"]},"executionInfo":{"elapsed":84620,"status":"ok","timestamp":1615271515605,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"tU5CUlJha0sj","outputId":"caaa2be0-5e26-4a8a-c447-6d8611ca06a5"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"476813c9ecc848b6a9ca8e8df4f80332","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0775239b0f194885a8c4c1ad299240f4","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n","  )\n",")"]},"execution_count":25,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from transformers import XLMRobertaForSequenceClassification, AdamW, XLMRobertaConfig\n","\n","# Load XLMRobertaForSequenceClassification, the pretrained XLMRoberta model with a single \n","# linear classification layer on top. \n","model = XLMRobertaForSequenceClassification.from_pretrained(\n","    \"xlm-roberta-base\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 5, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83842,"status":"ok","timestamp":1615271515605,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"xG1WZVaYa0sj","outputId":"ba84a8e5-8037-4e7a-f73f-3203a4baa753"},"outputs":[{"name":"stdout","output_type":"stream","text":["The XLM-RoBERTa model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","roberta.embeddings.word_embeddings.weight               (250002, 768)\n","roberta.embeddings.position_embeddings.weight             (514, 768)\n","roberta.embeddings.token_type_embeddings.weight             (1, 768)\n","roberta.embeddings.LayerNorm.weight                           (768,)\n","roberta.embeddings.LayerNorm.bias                             (768,)\n","\n","==== First Transformer ====\n","\n","roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n","roberta.encoder.layer.0.attention.self.query.bias             (768,)\n","roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n","roberta.encoder.layer.0.attention.self.key.bias               (768,)\n","roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n","roberta.encoder.layer.0.attention.self.value.bias             (768,)\n","roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n","roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n","roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n","roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n","roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n","roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n","roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n","roberta.encoder.layer.0.output.dense.bias                     (768,)\n","roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n","roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n","\n","==== Output Layer ====\n","\n","classifier.dense.weight                                   (768, 768)\n","classifier.dense.bias                                         (768,)\n","classifier.out_proj.weight                                  (5, 768)\n","classifier.out_proj.bias                                        (5,)\n"]}],"source":["# Check the model's parameter\n","# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The XLM-RoBERTa model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"markdown","metadata":{"id":"3aS4ZD9da0sj"},"source":["## Optimizer & Learning Rate Scheduler\n","\n","Batch size: 16, 32 (We chose 32 when creating our DataLoaders).\n","\n","Learning rate (Adam): 5e-5, 3e-5, 2e-5 (We'll use 2e-5).\n","\n","Number of epochs: 2, 3, 4 (We'll use 4)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BtN8Sh3a0sj"},"outputs":[],"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81959,"status":"ok","timestamp":1615271515606,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"rbHpIFCta0sk","outputId":"f89f00b2-bb9e-4ce9-9e12-d1d0351707eb"},"outputs":[{"data":{"text/plain":["<torch.optim.lr_scheduler.LambdaLR at 0x7fcb5b53c390>"]},"execution_count":28,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","scheduler"]},{"cell_type":"markdown","metadata":{"id":"llt00dDna0sk"},"source":["## Training!\n","Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a training phase and a validation phase. At each pass we need to:\n","\n","Training loop:\n","- Unpack our data inputs and labels\n","- Load data onto the GPU for acceleration\n","- Clear out the gradients calculated in the previous pass. \n","    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n","- Forward pass (feed input data through the network)\n","- Backward pass (backpropagation)\n","- Tell the network to update parameters with optimizer.step()\n","- Track variables for monitoring progress\n","\n","Evaluation loop:\n","- Unpack our data inputs and labels\n","- Load data onto the GPU for acceleration\n","- Forward pass (feed input data through the network)\n","- Compute loss on our validation data and track variables for monitoring progress\n","\n","So please read carefully through the comments to get an understanding of what's happening. If you're unfamiliar with pytorch a quick look at some of their [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) will help show you that training loops really involve only a few simple steps; the rest is usually just decoration and logging.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkjtEZ5fa0sk"},"outputs":[],"source":["# Define a helper function for calculating accuracy.\n","\n","import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTLFVyv3a0sk"},"outputs":[],"source":["#Helper function for formatting elapsed times.\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5549991,"status":"ok","timestamp":1615276984962,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"D2YmkmTxa0sk","outputId":"b27267d8-1223-463d-a734-bb3c13ffb2fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of  5,625.    Elapsed: 0:00:10.\n","  Batch    80  of  5,625.    Elapsed: 0:00:19.\n","  Batch   120  of  5,625.    Elapsed: 0:00:29.\n","  Batch   160  of  5,625.    Elapsed: 0:00:38.\n","  Batch   200  of  5,625.    Elapsed: 0:00:47.\n","  Batch   240  of  5,625.    Elapsed: 0:00:57.\n","  Batch   280  of  5,625.    Elapsed: 0:01:06.\n","  Batch   320  of  5,625.    Elapsed: 0:01:16.\n","  Batch   360  of  5,625.    Elapsed: 0:01:25.\n","  Batch   400  of  5,625.    Elapsed: 0:01:35.\n","  Batch   440  of  5,625.    Elapsed: 0:01:44.\n","  Batch   480  of  5,625.    Elapsed: 0:01:54.\n","  Batch   520  of  5,625.    Elapsed: 0:02:03.\n","  Batch   560  of  5,625.    Elapsed: 0:02:12.\n","  Batch   600  of  5,625.    Elapsed: 0:02:22.\n","  Batch   640  of  5,625.    Elapsed: 0:02:31.\n","  Batch   680  of  5,625.    Elapsed: 0:02:41.\n","  Batch   720  of  5,625.    Elapsed: 0:02:50.\n","  Batch   760  of  5,625.    Elapsed: 0:03:00.\n","  Batch   800  of  5,625.    Elapsed: 0:03:09.\n","  Batch   840  of  5,625.    Elapsed: 0:03:19.\n","  Batch   880  of  5,625.    Elapsed: 0:03:28.\n","  Batch   920  of  5,625.    Elapsed: 0:03:38.\n","  Batch   960  of  5,625.    Elapsed: 0:03:47.\n","  Batch 1,000  of  5,625.    Elapsed: 0:03:56.\n","  Batch 1,040  of  5,625.    Elapsed: 0:04:06.\n","  Batch 1,080  of  5,625.    Elapsed: 0:04:15.\n","  Batch 1,120  of  5,625.    Elapsed: 0:04:25.\n","  Batch 1,160  of  5,625.    Elapsed: 0:04:34.\n","  Batch 1,200  of  5,625.    Elapsed: 0:04:44.\n","  Batch 1,240  of  5,625.    Elapsed: 0:04:53.\n","  Batch 1,280  of  5,625.    Elapsed: 0:05:03.\n","  Batch 1,320  of  5,625.    Elapsed: 0:05:12.\n","  Batch 1,360  of  5,625.    Elapsed: 0:05:21.\n","  Batch 1,400  of  5,625.    Elapsed: 0:05:31.\n","  Batch 1,440  of  5,625.    Elapsed: 0:05:40.\n","  Batch 1,480  of  5,625.    Elapsed: 0:05:50.\n","  Batch 1,520  of  5,625.    Elapsed: 0:05:59.\n","  Batch 1,560  of  5,625.    Elapsed: 0:06:09.\n","  Batch 1,600  of  5,625.    Elapsed: 0:06:18.\n","  Batch 1,640  of  5,625.    Elapsed: 0:06:28.\n","  Batch 1,680  of  5,625.    Elapsed: 0:06:37.\n","  Batch 1,720  of  5,625.    Elapsed: 0:06:46.\n","  Batch 1,760  of  5,625.    Elapsed: 0:06:56.\n","  Batch 1,800  of  5,625.    Elapsed: 0:07:05.\n","  Batch 1,840  of  5,625.    Elapsed: 0:07:15.\n","  Batch 1,880  of  5,625.    Elapsed: 0:07:24.\n","  Batch 1,920  of  5,625.    Elapsed: 0:07:34.\n","  Batch 1,960  of  5,625.    Elapsed: 0:07:43.\n","  Batch 2,000  of  5,625.    Elapsed: 0:07:52.\n","  Batch 2,040  of  5,625.    Elapsed: 0:08:02.\n","  Batch 2,080  of  5,625.    Elapsed: 0:08:11.\n","  Batch 2,120  of  5,625.    Elapsed: 0:08:21.\n","  Batch 2,160  of  5,625.    Elapsed: 0:08:30.\n","  Batch 2,200  of  5,625.    Elapsed: 0:08:40.\n","  Batch 2,240  of  5,625.    Elapsed: 0:08:49.\n","  Batch 2,280  of  5,625.    Elapsed: 0:08:59.\n","  Batch 2,320  of  5,625.    Elapsed: 0:09:08.\n","  Batch 2,360  of  5,625.    Elapsed: 0:09:17.\n","  Batch 2,400  of  5,625.    Elapsed: 0:09:27.\n","  Batch 2,440  of  5,625.    Elapsed: 0:09:36.\n","  Batch 2,480  of  5,625.    Elapsed: 0:09:46.\n","  Batch 2,520  of  5,625.    Elapsed: 0:09:55.\n","  Batch 2,560  of  5,625.    Elapsed: 0:10:05.\n","  Batch 2,600  of  5,625.    Elapsed: 0:10:14.\n","  Batch 2,640  of  5,625.    Elapsed: 0:10:23.\n","  Batch 2,680  of  5,625.    Elapsed: 0:10:33.\n","  Batch 2,720  of  5,625.    Elapsed: 0:10:42.\n","  Batch 2,760  of  5,625.    Elapsed: 0:10:52.\n","  Batch 2,800  of  5,625.    Elapsed: 0:11:01.\n","  Batch 2,840  of  5,625.    Elapsed: 0:11:11.\n","  Batch 2,880  of  5,625.    Elapsed: 0:11:20.\n","  Batch 2,920  of  5,625.    Elapsed: 0:11:30.\n","  Batch 2,960  of  5,625.    Elapsed: 0:11:39.\n","  Batch 3,000  of  5,625.    Elapsed: 0:11:48.\n","  Batch 3,040  of  5,625.    Elapsed: 0:11:58.\n","  Batch 3,080  of  5,625.    Elapsed: 0:12:07.\n","  Batch 3,120  of  5,625.    Elapsed: 0:12:17.\n","  Batch 3,160  of  5,625.    Elapsed: 0:12:26.\n","  Batch 3,200  of  5,625.    Elapsed: 0:12:36.\n","  Batch 3,240  of  5,625.    Elapsed: 0:12:45.\n","  Batch 3,280  of  5,625.    Elapsed: 0:12:55.\n","  Batch 3,320  of  5,625.    Elapsed: 0:13:04.\n","  Batch 3,360  of  5,625.    Elapsed: 0:13:14.\n","  Batch 3,400  of  5,625.    Elapsed: 0:13:23.\n","  Batch 3,440  of  5,625.    Elapsed: 0:13:32.\n","  Batch 3,480  of  5,625.    Elapsed: 0:13:42.\n","  Batch 3,520  of  5,625.    Elapsed: 0:13:51.\n","  Batch 3,560  of  5,625.    Elapsed: 0:14:01.\n","  Batch 3,600  of  5,625.    Elapsed: 0:14:10.\n","  Batch 3,640  of  5,625.    Elapsed: 0:14:20.\n","  Batch 3,680  of  5,625.    Elapsed: 0:14:29.\n","  Batch 3,720  of  5,625.    Elapsed: 0:14:38.\n","  Batch 3,760  of  5,625.    Elapsed: 0:14:48.\n","  Batch 3,800  of  5,625.    Elapsed: 0:14:57.\n","  Batch 3,840  of  5,625.    Elapsed: 0:15:07.\n","  Batch 3,880  of  5,625.    Elapsed: 0:15:16.\n","  Batch 3,920  of  5,625.    Elapsed: 0:15:26.\n","  Batch 3,960  of  5,625.    Elapsed: 0:15:35.\n","  Batch 4,000  of  5,625.    Elapsed: 0:15:45.\n","  Batch 4,040  of  5,625.    Elapsed: 0:15:54.\n","  Batch 4,080  of  5,625.    Elapsed: 0:16:03.\n","  Batch 4,120  of  5,625.    Elapsed: 0:16:13.\n","  Batch 4,160  of  5,625.    Elapsed: 0:16:22.\n","  Batch 4,200  of  5,625.    Elapsed: 0:16:32.\n","  Batch 4,240  of  5,625.    Elapsed: 0:16:41.\n","  Batch 4,280  of  5,625.    Elapsed: 0:16:51.\n","  Batch 4,320  of  5,625.    Elapsed: 0:17:00.\n","  Batch 4,360  of  5,625.    Elapsed: 0:17:10.\n","  Batch 4,400  of  5,625.    Elapsed: 0:17:19.\n","  Batch 4,440  of  5,625.    Elapsed: 0:17:28.\n","  Batch 4,480  of  5,625.    Elapsed: 0:17:38.\n","  Batch 4,520  of  5,625.    Elapsed: 0:17:47.\n","  Batch 4,560  of  5,625.    Elapsed: 0:17:57.\n","  Batch 4,600  of  5,625.    Elapsed: 0:18:06.\n","  Batch 4,640  of  5,625.    Elapsed: 0:18:16.\n","  Batch 4,680  of  5,625.    Elapsed: 0:18:25.\n","  Batch 4,720  of  5,625.    Elapsed: 0:18:35.\n","  Batch 4,760  of  5,625.    Elapsed: 0:18:44.\n","  Batch 4,800  of  5,625.    Elapsed: 0:18:53.\n","  Batch 4,840  of  5,625.    Elapsed: 0:19:03.\n","  Batch 4,880  of  5,625.    Elapsed: 0:19:12.\n","  Batch 4,920  of  5,625.    Elapsed: 0:19:22.\n","  Batch 4,960  of  5,625.    Elapsed: 0:19:31.\n","  Batch 5,000  of  5,625.    Elapsed: 0:19:41.\n","  Batch 5,040  of  5,625.    Elapsed: 0:19:50.\n","  Batch 5,080  of  5,625.    Elapsed: 0:20:00.\n","  Batch 5,120  of  5,625.    Elapsed: 0:20:09.\n","  Batch 5,160  of  5,625.    Elapsed: 0:20:18.\n","  Batch 5,200  of  5,625.    Elapsed: 0:20:28.\n","  Batch 5,240  of  5,625.    Elapsed: 0:20:37.\n","  Batch 5,280  of  5,625.    Elapsed: 0:20:47.\n","  Batch 5,320  of  5,625.    Elapsed: 0:20:56.\n","  Batch 5,360  of  5,625.    Elapsed: 0:21:06.\n","  Batch 5,400  of  5,625.    Elapsed: 0:21:15.\n","  Batch 5,440  of  5,625.    Elapsed: 0:21:25.\n","  Batch 5,480  of  5,625.    Elapsed: 0:21:34.\n","  Batch 5,520  of  5,625.    Elapsed: 0:21:44.\n","  Batch 5,560  of  5,625.    Elapsed: 0:21:53.\n","  Batch 5,600  of  5,625.    Elapsed: 0:22:03.\n","\n","  Average training loss: 1.07\n","  Training epoch took: 0:22:08\n","\n","Running Validation...\n","  Accuracy: 0.57\n","  Validation took: 0:00:38\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of  5,625.    Elapsed: 0:00:09.\n","  Batch    80  of  5,625.    Elapsed: 0:00:19.\n","  Batch   120  of  5,625.    Elapsed: 0:00:28.\n","  Batch   160  of  5,625.    Elapsed: 0:00:38.\n","  Batch   200  of  5,625.    Elapsed: 0:00:47.\n","  Batch   240  of  5,625.    Elapsed: 0:00:57.\n","  Batch   280  of  5,625.    Elapsed: 0:01:06.\n","  Batch   320  of  5,625.    Elapsed: 0:01:16.\n","  Batch   360  of  5,625.    Elapsed: 0:01:25.\n","  Batch   400  of  5,625.    Elapsed: 0:01:35.\n","  Batch   440  of  5,625.    Elapsed: 0:01:44.\n","  Batch   480  of  5,625.    Elapsed: 0:01:53.\n","  Batch   520  of  5,625.    Elapsed: 0:02:03.\n","  Batch   560  of  5,625.    Elapsed: 0:02:12.\n","  Batch   600  of  5,625.    Elapsed: 0:02:22.\n","  Batch   640  of  5,625.    Elapsed: 0:02:31.\n","  Batch   680  of  5,625.    Elapsed: 0:02:41.\n","  Batch   720  of  5,625.    Elapsed: 0:02:50.\n","  Batch   760  of  5,625.    Elapsed: 0:03:00.\n","  Batch   800  of  5,625.    Elapsed: 0:03:09.\n","  Batch   840  of  5,625.    Elapsed: 0:03:19.\n","  Batch   880  of  5,625.    Elapsed: 0:03:28.\n","  Batch   920  of  5,625.    Elapsed: 0:03:38.\n","  Batch   960  of  5,625.    Elapsed: 0:03:47.\n","  Batch 1,000  of  5,625.    Elapsed: 0:03:56.\n","  Batch 1,040  of  5,625.    Elapsed: 0:04:06.\n","  Batch 1,080  of  5,625.    Elapsed: 0:04:15.\n","  Batch 1,120  of  5,625.    Elapsed: 0:04:25.\n","  Batch 1,160  of  5,625.    Elapsed: 0:04:34.\n","  Batch 1,200  of  5,625.    Elapsed: 0:04:44.\n","  Batch 1,240  of  5,625.    Elapsed: 0:04:53.\n","  Batch 1,280  of  5,625.    Elapsed: 0:05:03.\n","  Batch 1,320  of  5,625.    Elapsed: 0:05:12.\n","  Batch 1,360  of  5,625.    Elapsed: 0:05:22.\n","  Batch 1,400  of  5,625.    Elapsed: 0:05:31.\n","  Batch 1,440  of  5,625.    Elapsed: 0:05:40.\n","  Batch 1,480  of  5,625.    Elapsed: 0:05:50.\n","  Batch 1,520  of  5,625.    Elapsed: 0:05:59.\n","  Batch 1,560  of  5,625.    Elapsed: 0:06:09.\n","  Batch 1,600  of  5,625.    Elapsed: 0:06:18.\n","  Batch 1,640  of  5,625.    Elapsed: 0:06:28.\n","  Batch 1,680  of  5,625.    Elapsed: 0:06:37.\n","  Batch 1,720  of  5,625.    Elapsed: 0:06:47.\n","  Batch 1,760  of  5,625.    Elapsed: 0:06:56.\n","  Batch 1,800  of  5,625.    Elapsed: 0:07:05.\n","  Batch 1,840  of  5,625.    Elapsed: 0:07:15.\n","  Batch 1,880  of  5,625.    Elapsed: 0:07:24.\n","  Batch 1,920  of  5,625.    Elapsed: 0:07:34.\n","  Batch 1,960  of  5,625.    Elapsed: 0:07:43.\n","  Batch 2,000  of  5,625.    Elapsed: 0:07:53.\n","  Batch 2,040  of  5,625.    Elapsed: 0:08:02.\n","  Batch 2,080  of  5,625.    Elapsed: 0:08:12.\n","  Batch 2,120  of  5,625.    Elapsed: 0:08:21.\n","  Batch 2,160  of  5,625.    Elapsed: 0:08:31.\n","  Batch 2,200  of  5,625.    Elapsed: 0:08:40.\n","  Batch 2,240  of  5,625.    Elapsed: 0:08:49.\n","  Batch 2,280  of  5,625.    Elapsed: 0:08:59.\n","  Batch 2,320  of  5,625.    Elapsed: 0:09:08.\n","  Batch 2,360  of  5,625.    Elapsed: 0:09:18.\n","  Batch 2,400  of  5,625.    Elapsed: 0:09:27.\n","  Batch 2,440  of  5,625.    Elapsed: 0:09:37.\n","  Batch 2,480  of  5,625.    Elapsed: 0:09:46.\n","  Batch 2,520  of  5,625.    Elapsed: 0:09:56.\n","  Batch 2,560  of  5,625.    Elapsed: 0:10:05.\n","  Batch 2,600  of  5,625.    Elapsed: 0:10:15.\n","  Batch 2,640  of  5,625.    Elapsed: 0:10:24.\n","  Batch 2,680  of  5,625.    Elapsed: 0:10:33.\n","  Batch 2,720  of  5,625.    Elapsed: 0:10:43.\n","  Batch 2,760  of  5,625.    Elapsed: 0:10:52.\n","  Batch 2,800  of  5,625.    Elapsed: 0:11:02.\n","  Batch 2,840  of  5,625.    Elapsed: 0:11:11.\n","  Batch 2,880  of  5,625.    Elapsed: 0:11:21.\n","  Batch 2,920  of  5,625.    Elapsed: 0:11:30.\n","  Batch 2,960  of  5,625.    Elapsed: 0:11:40.\n","  Batch 3,000  of  5,625.    Elapsed: 0:11:49.\n","  Batch 3,040  of  5,625.    Elapsed: 0:11:59.\n","  Batch 3,080  of  5,625.    Elapsed: 0:12:08.\n","  Batch 3,120  of  5,625.    Elapsed: 0:12:18.\n","  Batch 3,160  of  5,625.    Elapsed: 0:12:27.\n","  Batch 3,200  of  5,625.    Elapsed: 0:12:36.\n","  Batch 3,240  of  5,625.    Elapsed: 0:12:46.\n","  Batch 3,280  of  5,625.    Elapsed: 0:12:55.\n","  Batch 3,320  of  5,625.    Elapsed: 0:13:05.\n","  Batch 3,360  of  5,625.    Elapsed: 0:13:14.\n","  Batch 3,400  of  5,625.    Elapsed: 0:13:24.\n","  Batch 3,440  of  5,625.    Elapsed: 0:13:33.\n","  Batch 3,480  of  5,625.    Elapsed: 0:13:43.\n","  Batch 3,520  of  5,625.    Elapsed: 0:13:52.\n","  Batch 3,560  of  5,625.    Elapsed: 0:14:01.\n","  Batch 3,600  of  5,625.    Elapsed: 0:14:11.\n","  Batch 3,640  of  5,625.    Elapsed: 0:14:20.\n","  Batch 3,680  of  5,625.    Elapsed: 0:14:30.\n","  Batch 3,720  of  5,625.    Elapsed: 0:14:39.\n","  Batch 3,760  of  5,625.    Elapsed: 0:14:49.\n","  Batch 3,800  of  5,625.    Elapsed: 0:14:58.\n","  Batch 3,840  of  5,625.    Elapsed: 0:15:08.\n","  Batch 3,880  of  5,625.    Elapsed: 0:15:17.\n","  Batch 3,920  of  5,625.    Elapsed: 0:15:26.\n","  Batch 3,960  of  5,625.    Elapsed: 0:15:36.\n","  Batch 4,000  of  5,625.    Elapsed: 0:15:45.\n","  Batch 4,040  of  5,625.    Elapsed: 0:15:55.\n","  Batch 4,080  of  5,625.    Elapsed: 0:16:04.\n","  Batch 4,120  of  5,625.    Elapsed: 0:16:14.\n","  Batch 4,160  of  5,625.    Elapsed: 0:16:23.\n","  Batch 4,200  of  5,625.    Elapsed: 0:16:33.\n","  Batch 4,240  of  5,625.    Elapsed: 0:16:42.\n","  Batch 4,280  of  5,625.    Elapsed: 0:16:52.\n","  Batch 4,320  of  5,625.    Elapsed: 0:17:01.\n","  Batch 4,360  of  5,625.    Elapsed: 0:17:10.\n","  Batch 4,400  of  5,625.    Elapsed: 0:17:20.\n","  Batch 4,440  of  5,625.    Elapsed: 0:17:29.\n","  Batch 4,480  of  5,625.    Elapsed: 0:17:39.\n","  Batch 4,520  of  5,625.    Elapsed: 0:17:48.\n","  Batch 4,560  of  5,625.    Elapsed: 0:17:58.\n","  Batch 4,600  of  5,625.    Elapsed: 0:18:07.\n","  Batch 4,640  of  5,625.    Elapsed: 0:18:17.\n","  Batch 4,680  of  5,625.    Elapsed: 0:18:26.\n","  Batch 4,720  of  5,625.    Elapsed: 0:18:35.\n","  Batch 4,760  of  5,625.    Elapsed: 0:18:45.\n","  Batch 4,800  of  5,625.    Elapsed: 0:18:54.\n","  Batch 4,840  of  5,625.    Elapsed: 0:19:04.\n","  Batch 4,880  of  5,625.    Elapsed: 0:19:13.\n","  Batch 4,920  of  5,625.    Elapsed: 0:19:23.\n","  Batch 4,960  of  5,625.    Elapsed: 0:19:32.\n","  Batch 5,000  of  5,625.    Elapsed: 0:19:42.\n","  Batch 5,040  of  5,625.    Elapsed: 0:19:51.\n","  Batch 5,080  of  5,625.    Elapsed: 0:20:00.\n","  Batch 5,120  of  5,625.    Elapsed: 0:20:10.\n","  Batch 5,160  of  5,625.    Elapsed: 0:20:19.\n","  Batch 5,200  of  5,625.    Elapsed: 0:20:29.\n","  Batch 5,240  of  5,625.    Elapsed: 0:20:38.\n","  Batch 5,280  of  5,625.    Elapsed: 0:20:48.\n","  Batch 5,320  of  5,625.    Elapsed: 0:20:57.\n","  Batch 5,360  of  5,625.    Elapsed: 0:21:07.\n","  Batch 5,400  of  5,625.    Elapsed: 0:21:16.\n","  Batch 5,440  of  5,625.    Elapsed: 0:21:26.\n","  Batch 5,480  of  5,625.    Elapsed: 0:21:35.\n","  Batch 5,520  of  5,625.    Elapsed: 0:21:44.\n","  Batch 5,560  of  5,625.    Elapsed: 0:21:54.\n","  Batch 5,600  of  5,625.    Elapsed: 0:22:03.\n","\n","  Average training loss: 0.96\n","  Training epoch took: 0:22:09\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation took: 0:00:38\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of  5,625.    Elapsed: 0:00:09.\n","  Batch    80  of  5,625.    Elapsed: 0:00:19.\n","  Batch   120  of  5,625.    Elapsed: 0:00:28.\n","  Batch   160  of  5,625.    Elapsed: 0:00:38.\n","  Batch   200  of  5,625.    Elapsed: 0:00:47.\n","  Batch   240  of  5,625.    Elapsed: 0:00:57.\n","  Batch   280  of  5,625.    Elapsed: 0:01:06.\n","  Batch   320  of  5,625.    Elapsed: 0:01:16.\n","  Batch   360  of  5,625.    Elapsed: 0:01:25.\n","  Batch   400  of  5,625.    Elapsed: 0:01:34.\n","  Batch   440  of  5,625.    Elapsed: 0:01:44.\n","  Batch   480  of  5,625.    Elapsed: 0:01:53.\n","  Batch   520  of  5,625.    Elapsed: 0:02:03.\n","  Batch   560  of  5,625.    Elapsed: 0:02:12.\n","  Batch   600  of  5,625.    Elapsed: 0:02:22.\n","  Batch   640  of  5,625.    Elapsed: 0:02:31.\n","  Batch   680  of  5,625.    Elapsed: 0:02:41.\n","  Batch   720  of  5,625.    Elapsed: 0:02:50.\n","  Batch   760  of  5,625.    Elapsed: 0:02:59.\n","  Batch   800  of  5,625.    Elapsed: 0:03:09.\n","  Batch   840  of  5,625.    Elapsed: 0:03:18.\n","  Batch   880  of  5,625.    Elapsed: 0:03:28.\n","  Batch   920  of  5,625.    Elapsed: 0:03:37.\n","  Batch   960  of  5,625.    Elapsed: 0:03:47.\n","  Batch 1,000  of  5,625.    Elapsed: 0:03:56.\n","  Batch 1,040  of  5,625.    Elapsed: 0:04:06.\n","  Batch 1,080  of  5,625.    Elapsed: 0:04:15.\n","  Batch 1,120  of  5,625.    Elapsed: 0:04:25.\n","  Batch 1,160  of  5,625.    Elapsed: 0:04:34.\n","  Batch 1,200  of  5,625.    Elapsed: 0:04:43.\n","  Batch 1,240  of  5,625.    Elapsed: 0:04:53.\n","  Batch 1,280  of  5,625.    Elapsed: 0:05:02.\n","  Batch 1,320  of  5,625.    Elapsed: 0:05:12.\n","  Batch 1,360  of  5,625.    Elapsed: 0:05:21.\n","  Batch 1,400  of  5,625.    Elapsed: 0:05:31.\n","  Batch 1,440  of  5,625.    Elapsed: 0:05:40.\n","  Batch 1,480  of  5,625.    Elapsed: 0:05:50.\n","  Batch 1,520  of  5,625.    Elapsed: 0:05:59.\n","  Batch 1,560  of  5,625.    Elapsed: 0:06:08.\n","  Batch 1,600  of  5,625.    Elapsed: 0:06:18.\n","  Batch 1,640  of  5,625.    Elapsed: 0:06:27.\n","  Batch 1,680  of  5,625.    Elapsed: 0:06:37.\n","  Batch 1,720  of  5,625.    Elapsed: 0:06:46.\n","  Batch 1,760  of  5,625.    Elapsed: 0:06:56.\n","  Batch 1,800  of  5,625.    Elapsed: 0:07:05.\n","  Batch 1,840  of  5,625.    Elapsed: 0:07:15.\n","  Batch 1,880  of  5,625.    Elapsed: 0:07:24.\n","  Batch 1,920  of  5,625.    Elapsed: 0:07:34.\n","  Batch 1,960  of  5,625.    Elapsed: 0:07:43.\n","  Batch 2,000  of  5,625.    Elapsed: 0:07:53.\n","  Batch 2,040  of  5,625.    Elapsed: 0:08:02.\n","  Batch 2,080  of  5,625.    Elapsed: 0:08:11.\n","  Batch 2,120  of  5,625.    Elapsed: 0:08:21.\n","  Batch 2,160  of  5,625.    Elapsed: 0:08:30.\n","  Batch 2,200  of  5,625.    Elapsed: 0:08:40.\n","  Batch 2,240  of  5,625.    Elapsed: 0:08:49.\n","  Batch 2,280  of  5,625.    Elapsed: 0:08:59.\n","  Batch 2,320  of  5,625.    Elapsed: 0:09:08.\n","  Batch 2,360  of  5,625.    Elapsed: 0:09:18.\n","  Batch 2,400  of  5,625.    Elapsed: 0:09:27.\n","  Batch 2,440  of  5,625.    Elapsed: 0:09:36.\n","  Batch 2,480  of  5,625.    Elapsed: 0:09:46.\n","  Batch 2,520  of  5,625.    Elapsed: 0:09:55.\n","  Batch 2,560  of  5,625.    Elapsed: 0:10:05.\n","  Batch 2,600  of  5,625.    Elapsed: 0:10:14.\n","  Batch 2,640  of  5,625.    Elapsed: 0:10:24.\n","  Batch 2,680  of  5,625.    Elapsed: 0:10:33.\n","  Batch 2,720  of  5,625.    Elapsed: 0:10:43.\n","  Batch 2,760  of  5,625.    Elapsed: 0:10:52.\n","  Batch 2,800  of  5,625.    Elapsed: 0:11:02.\n","  Batch 2,840  of  5,625.    Elapsed: 0:11:11.\n","  Batch 2,880  of  5,625.    Elapsed: 0:11:20.\n","  Batch 2,920  of  5,625.    Elapsed: 0:11:30.\n","  Batch 2,960  of  5,625.    Elapsed: 0:11:39.\n","  Batch 3,000  of  5,625.    Elapsed: 0:11:49.\n","  Batch 3,040  of  5,625.    Elapsed: 0:11:58.\n","  Batch 3,080  of  5,625.    Elapsed: 0:12:08.\n","  Batch 3,120  of  5,625.    Elapsed: 0:12:17.\n","  Batch 3,160  of  5,625.    Elapsed: 0:12:27.\n","  Batch 3,200  of  5,625.    Elapsed: 0:12:36.\n","  Batch 3,240  of  5,625.    Elapsed: 0:12:45.\n","  Batch 3,280  of  5,625.    Elapsed: 0:12:55.\n","  Batch 3,320  of  5,625.    Elapsed: 0:13:04.\n","  Batch 3,360  of  5,625.    Elapsed: 0:13:14.\n","  Batch 3,400  of  5,625.    Elapsed: 0:13:23.\n","  Batch 3,440  of  5,625.    Elapsed: 0:13:33.\n","  Batch 3,480  of  5,625.    Elapsed: 0:13:42.\n","  Batch 3,520  of  5,625.    Elapsed: 0:13:52.\n","  Batch 3,560  of  5,625.    Elapsed: 0:14:01.\n","  Batch 3,600  of  5,625.    Elapsed: 0:14:11.\n","  Batch 3,640  of  5,625.    Elapsed: 0:14:20.\n","  Batch 3,680  of  5,625.    Elapsed: 0:14:29.\n","  Batch 3,720  of  5,625.    Elapsed: 0:14:39.\n","  Batch 3,760  of  5,625.    Elapsed: 0:14:48.\n","  Batch 3,800  of  5,625.    Elapsed: 0:14:58.\n","  Batch 3,840  of  5,625.    Elapsed: 0:15:07.\n","  Batch 3,880  of  5,625.    Elapsed: 0:15:17.\n","  Batch 3,920  of  5,625.    Elapsed: 0:15:26.\n","  Batch 3,960  of  5,625.    Elapsed: 0:15:36.\n","  Batch 4,000  of  5,625.    Elapsed: 0:15:45.\n","  Batch 4,040  of  5,625.    Elapsed: 0:15:54.\n","  Batch 4,080  of  5,625.    Elapsed: 0:16:04.\n","  Batch 4,120  of  5,625.    Elapsed: 0:16:13.\n","  Batch 4,160  of  5,625.    Elapsed: 0:16:23.\n","  Batch 4,200  of  5,625.    Elapsed: 0:16:32.\n","  Batch 4,240  of  5,625.    Elapsed: 0:16:42.\n","  Batch 4,280  of  5,625.    Elapsed: 0:16:51.\n","  Batch 4,320  of  5,625.    Elapsed: 0:17:01.\n","  Batch 4,360  of  5,625.    Elapsed: 0:17:10.\n","  Batch 4,400  of  5,625.    Elapsed: 0:17:20.\n","  Batch 4,440  of  5,625.    Elapsed: 0:17:29.\n","  Batch 4,480  of  5,625.    Elapsed: 0:17:38.\n","  Batch 4,520  of  5,625.    Elapsed: 0:17:48.\n","  Batch 4,560  of  5,625.    Elapsed: 0:17:57.\n","  Batch 4,600  of  5,625.    Elapsed: 0:18:07.\n","  Batch 4,640  of  5,625.    Elapsed: 0:18:16.\n","  Batch 4,680  of  5,625.    Elapsed: 0:18:26.\n","  Batch 4,720  of  5,625.    Elapsed: 0:18:35.\n","  Batch 4,760  of  5,625.    Elapsed: 0:18:45.\n","  Batch 4,800  of  5,625.    Elapsed: 0:18:54.\n","  Batch 4,840  of  5,625.    Elapsed: 0:19:04.\n","  Batch 4,880  of  5,625.    Elapsed: 0:19:13.\n","  Batch 4,920  of  5,625.    Elapsed: 0:19:22.\n","  Batch 4,960  of  5,625.    Elapsed: 0:19:32.\n","  Batch 5,000  of  5,625.    Elapsed: 0:19:41.\n","  Batch 5,040  of  5,625.    Elapsed: 0:19:51.\n","  Batch 5,080  of  5,625.    Elapsed: 0:20:00.\n","  Batch 5,120  of  5,625.    Elapsed: 0:20:10.\n","  Batch 5,160  of  5,625.    Elapsed: 0:20:19.\n","  Batch 5,200  of  5,625.    Elapsed: 0:20:29.\n","  Batch 5,240  of  5,625.    Elapsed: 0:20:38.\n","  Batch 5,280  of  5,625.    Elapsed: 0:20:47.\n","  Batch 5,320  of  5,625.    Elapsed: 0:20:57.\n","  Batch 5,360  of  5,625.    Elapsed: 0:21:06.\n","  Batch 5,400  of  5,625.    Elapsed: 0:21:16.\n","  Batch 5,440  of  5,625.    Elapsed: 0:21:25.\n","  Batch 5,480  of  5,625.    Elapsed: 0:21:35.\n","  Batch 5,520  of  5,625.    Elapsed: 0:21:44.\n","  Batch 5,560  of  5,625.    Elapsed: 0:21:54.\n","  Batch 5,600  of  5,625.    Elapsed: 0:22:03.\n","\n","  Average training loss: 0.89\n","  Training epoch took: 0:22:09\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation took: 0:00:38\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of  5,625.    Elapsed: 0:00:09.\n","  Batch    80  of  5,625.    Elapsed: 0:00:19.\n","  Batch   120  of  5,625.    Elapsed: 0:00:28.\n","  Batch   160  of  5,625.    Elapsed: 0:00:38.\n","  Batch   200  of  5,625.    Elapsed: 0:00:47.\n","  Batch   240  of  5,625.    Elapsed: 0:00:57.\n","  Batch   280  of  5,625.    Elapsed: 0:01:06.\n","  Batch   320  of  5,625.    Elapsed: 0:01:16.\n","  Batch   360  of  5,625.    Elapsed: 0:01:25.\n","  Batch   400  of  5,625.    Elapsed: 0:01:34.\n","  Batch   440  of  5,625.    Elapsed: 0:01:44.\n","  Batch   480  of  5,625.    Elapsed: 0:01:53.\n","  Batch   520  of  5,625.    Elapsed: 0:02:03.\n","  Batch   560  of  5,625.    Elapsed: 0:02:12.\n","  Batch   600  of  5,625.    Elapsed: 0:02:22.\n","  Batch   640  of  5,625.    Elapsed: 0:02:31.\n","  Batch   680  of  5,625.    Elapsed: 0:02:41.\n","  Batch   720  of  5,625.    Elapsed: 0:02:50.\n","  Batch   760  of  5,625.    Elapsed: 0:02:59.\n","  Batch   800  of  5,625.    Elapsed: 0:03:09.\n","  Batch   840  of  5,625.    Elapsed: 0:03:18.\n","  Batch   880  of  5,625.    Elapsed: 0:03:28.\n","  Batch   920  of  5,625.    Elapsed: 0:03:37.\n","  Batch   960  of  5,625.    Elapsed: 0:03:47.\n","  Batch 1,000  of  5,625.    Elapsed: 0:03:56.\n","  Batch 1,040  of  5,625.    Elapsed: 0:04:06.\n","  Batch 1,080  of  5,625.    Elapsed: 0:04:15.\n","  Batch 1,120  of  5,625.    Elapsed: 0:04:25.\n","  Batch 1,160  of  5,625.    Elapsed: 0:04:34.\n","  Batch 1,200  of  5,625.    Elapsed: 0:04:43.\n","  Batch 1,240  of  5,625.    Elapsed: 0:04:53.\n","  Batch 1,280  of  5,625.    Elapsed: 0:05:02.\n","  Batch 1,320  of  5,625.    Elapsed: 0:05:12.\n","  Batch 1,360  of  5,625.    Elapsed: 0:05:21.\n","  Batch 1,400  of  5,625.    Elapsed: 0:05:31.\n","  Batch 1,440  of  5,625.    Elapsed: 0:05:40.\n","  Batch 1,480  of  5,625.    Elapsed: 0:05:50.\n","  Batch 1,520  of  5,625.    Elapsed: 0:05:59.\n","  Batch 1,560  of  5,625.    Elapsed: 0:06:09.\n","  Batch 1,600  of  5,625.    Elapsed: 0:06:18.\n","  Batch 1,640  of  5,625.    Elapsed: 0:06:27.\n","  Batch 1,680  of  5,625.    Elapsed: 0:06:37.\n","  Batch 1,720  of  5,625.    Elapsed: 0:06:46.\n","  Batch 1,760  of  5,625.    Elapsed: 0:06:56.\n","  Batch 1,800  of  5,625.    Elapsed: 0:07:05.\n","  Batch 1,840  of  5,625.    Elapsed: 0:07:15.\n","  Batch 1,880  of  5,625.    Elapsed: 0:07:24.\n","  Batch 1,920  of  5,625.    Elapsed: 0:07:34.\n","  Batch 1,960  of  5,625.    Elapsed: 0:07:43.\n","  Batch 2,000  of  5,625.    Elapsed: 0:07:53.\n","  Batch 2,040  of  5,625.    Elapsed: 0:08:02.\n","  Batch 2,080  of  5,625.    Elapsed: 0:08:11.\n","  Batch 2,120  of  5,625.    Elapsed: 0:08:21.\n","  Batch 2,160  of  5,625.    Elapsed: 0:08:30.\n","  Batch 2,200  of  5,625.    Elapsed: 0:08:40.\n","  Batch 2,240  of  5,625.    Elapsed: 0:08:49.\n","  Batch 2,280  of  5,625.    Elapsed: 0:08:59.\n","  Batch 2,320  of  5,625.    Elapsed: 0:09:08.\n","  Batch 2,360  of  5,625.    Elapsed: 0:09:18.\n","  Batch 2,400  of  5,625.    Elapsed: 0:09:27.\n","  Batch 2,440  of  5,625.    Elapsed: 0:09:37.\n","  Batch 2,480  of  5,625.    Elapsed: 0:09:46.\n","  Batch 2,520  of  5,625.    Elapsed: 0:09:55.\n","  Batch 2,560  of  5,625.    Elapsed: 0:10:05.\n","  Batch 2,600  of  5,625.    Elapsed: 0:10:14.\n","  Batch 2,640  of  5,625.    Elapsed: 0:10:24.\n","  Batch 2,680  of  5,625.    Elapsed: 0:10:33.\n","  Batch 2,720  of  5,625.    Elapsed: 0:10:43.\n","  Batch 2,760  of  5,625.    Elapsed: 0:10:52.\n","  Batch 2,800  of  5,625.    Elapsed: 0:11:02.\n","  Batch 2,840  of  5,625.    Elapsed: 0:11:11.\n","  Batch 2,880  of  5,625.    Elapsed: 0:11:21.\n","  Batch 2,920  of  5,625.    Elapsed: 0:11:30.\n","  Batch 2,960  of  5,625.    Elapsed: 0:11:39.\n","  Batch 3,000  of  5,625.    Elapsed: 0:11:49.\n","  Batch 3,040  of  5,625.    Elapsed: 0:11:58.\n","  Batch 3,080  of  5,625.    Elapsed: 0:12:08.\n","  Batch 3,120  of  5,625.    Elapsed: 0:12:17.\n","  Batch 3,160  of  5,625.    Elapsed: 0:12:27.\n","  Batch 3,200  of  5,625.    Elapsed: 0:12:36.\n","  Batch 3,240  of  5,625.    Elapsed: 0:12:46.\n","  Batch 3,280  of  5,625.    Elapsed: 0:12:55.\n","  Batch 3,320  of  5,625.    Elapsed: 0:13:05.\n","  Batch 3,360  of  5,625.    Elapsed: 0:13:14.\n","  Batch 3,400  of  5,625.    Elapsed: 0:13:23.\n","  Batch 3,440  of  5,625.    Elapsed: 0:13:33.\n","  Batch 3,480  of  5,625.    Elapsed: 0:13:42.\n","  Batch 3,520  of  5,625.    Elapsed: 0:13:52.\n","  Batch 3,560  of  5,625.    Elapsed: 0:14:01.\n","  Batch 3,600  of  5,625.    Elapsed: 0:14:11.\n","  Batch 3,640  of  5,625.    Elapsed: 0:14:20.\n","  Batch 3,680  of  5,625.    Elapsed: 0:14:30.\n","  Batch 3,720  of  5,625.    Elapsed: 0:14:39.\n","  Batch 3,760  of  5,625.    Elapsed: 0:14:49.\n","  Batch 3,800  of  5,625.    Elapsed: 0:14:58.\n","  Batch 3,840  of  5,625.    Elapsed: 0:15:07.\n","  Batch 3,880  of  5,625.    Elapsed: 0:15:17.\n","  Batch 3,920  of  5,625.    Elapsed: 0:15:26.\n","  Batch 3,960  of  5,625.    Elapsed: 0:15:36.\n","  Batch 4,000  of  5,625.    Elapsed: 0:15:45.\n","  Batch 4,040  of  5,625.    Elapsed: 0:15:55.\n","  Batch 4,080  of  5,625.    Elapsed: 0:16:04.\n","  Batch 4,120  of  5,625.    Elapsed: 0:16:14.\n","  Batch 4,160  of  5,625.    Elapsed: 0:16:23.\n","  Batch 4,200  of  5,625.    Elapsed: 0:16:33.\n","  Batch 4,240  of  5,625.    Elapsed: 0:16:42.\n","  Batch 4,280  of  5,625.    Elapsed: 0:16:51.\n","  Batch 4,320  of  5,625.    Elapsed: 0:17:01.\n","  Batch 4,360  of  5,625.    Elapsed: 0:17:10.\n","  Batch 4,400  of  5,625.    Elapsed: 0:17:20.\n","  Batch 4,440  of  5,625.    Elapsed: 0:17:29.\n","  Batch 4,480  of  5,625.    Elapsed: 0:17:39.\n","  Batch 4,520  of  5,625.    Elapsed: 0:17:48.\n","  Batch 4,560  of  5,625.    Elapsed: 0:17:58.\n","  Batch 4,600  of  5,625.    Elapsed: 0:18:07.\n","  Batch 4,640  of  5,625.    Elapsed: 0:18:17.\n","  Batch 4,680  of  5,625.    Elapsed: 0:18:26.\n","  Batch 4,720  of  5,625.    Elapsed: 0:18:35.\n","  Batch 4,760  of  5,625.    Elapsed: 0:18:45.\n","  Batch 4,800  of  5,625.    Elapsed: 0:18:54.\n","  Batch 4,840  of  5,625.    Elapsed: 0:19:04.\n","  Batch 4,880  of  5,625.    Elapsed: 0:19:13.\n","  Batch 4,920  of  5,625.    Elapsed: 0:19:23.\n","  Batch 4,960  of  5,625.    Elapsed: 0:19:32.\n","  Batch 5,000  of  5,625.    Elapsed: 0:19:42.\n","  Batch 5,040  of  5,625.    Elapsed: 0:19:51.\n","  Batch 5,080  of  5,625.    Elapsed: 0:20:01.\n","  Batch 5,120  of  5,625.    Elapsed: 0:20:10.\n","  Batch 5,160  of  5,625.    Elapsed: 0:20:20.\n","  Batch 5,200  of  5,625.    Elapsed: 0:20:29.\n","  Batch 5,240  of  5,625.    Elapsed: 0:20:38.\n","  Batch 5,280  of  5,625.    Elapsed: 0:20:48.\n","  Batch 5,320  of  5,625.    Elapsed: 0:20:57.\n","  Batch 5,360  of  5,625.    Elapsed: 0:21:07.\n","  Batch 5,400  of  5,625.    Elapsed: 0:21:16.\n","  Batch 5,440  of  5,625.    Elapsed: 0:21:26.\n","  Batch 5,480  of  5,625.    Elapsed: 0:21:35.\n","  Batch 5,520  of  5,625.    Elapsed: 0:21:45.\n","  Batch 5,560  of  5,625.    Elapsed: 0:21:54.\n","  Batch 5,600  of  5,625.    Elapsed: 0:22:03.\n","\n","  Average training loss: 0.84\n","  Training epoch took: 0:22:09\n","\n","Running Validation...\n","  Accuracy: 0.59\n","  Validation took: 0:00:38\n","\n","Training complete!\n"]}],"source":["import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":5549994,"status":"ok","timestamp":1615276985597,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"dm3RrS9Ha0sl","outputId":"5f7ff526-9d8b-4c61-f872-95b8b33b546f"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(loss_values, 'b-o')\n","\n","# Label the plot.\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xKrtol_ra0so"},"source":["## Saving Our Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6640,"status":"ok","timestamp":1615276992280,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"rBTXea9Ca0so","outputId":"8074327b-c1fe-4142-a2c5-aaf380ba5538"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to ./model_save\n"]},{"data":{"text/plain":["('./model_save/tokenizer_config.json',\n"," './model_save/special_tokens_map.json',\n"," './model_save/sentencepiece.bpe.model',\n"," './model_save/added_tokens.json')"]},"execution_count":33,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './model_save'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6919,"status":"ok","timestamp":1615276992593,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"Wa7j4WZUa0sp","outputId":"95e13dc0-6994-4582-b54f-64feecb2d2c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 1091180K\n","-rw-r--r-- 1 root root       1K Mar  9 08:03 config.json\n","-rw-r--r-- 1 root root 1086212K Mar  9 08:03 pytorch_model.bin\n","-rw-r--r-- 1 root root    4951K Mar  9 08:03 sentencepiece.bpe.model\n","-rw-r--r-- 1 root root       1K Mar  9 08:03 special_tokens_map.json\n","-rw-r--r-- 1 root root       1K Mar  9 08:03 tokenizer_config.json\n"]}],"source":["!ls -l --block-size=K ./model_save/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7415,"status":"ok","timestamp":1615276993115,"user":{"displayName":"ANDRE RUSLI (10110110103)","photoUrl":"","userId":"11890750785389102535"},"user_tz":-540},"id":"JQtacAsga0sp","outputId":"12f46b39-45f6-450c-eb85-092b095205a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 1061M Mar  9 08:03 ./model_save/pytorch_model.bin\n"]}],"source":["!ls -l --block-size=M ./model_save/pytorch_model.bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WK3fIj32a0sp"},"outputs":[],"source":["# Copy the model files to a directory in your Google Drive.\n","!cp -r ./model_save/ \"./models/en-finegrained-model/\""]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMt/JZLmTzHQu9ij3ePK3d0","collapsed_sections":["AfeRDxkRrya9","ZEbiFo5JQx8G","4PKM2b2qud9I","vtkPugaLtEZB","JGFYgb0-uZX8","5DfqXLf1uzmB","UHFTXw5FvP8H","siHMmP1Uv89y","Y3cIbRRHwJrW","XTzZYSeXwPEC","LfNROv8exiQ_","Beu4wAHyxqd3","-nLd0sJLVjSE","wUkQol0ra0sa"],"name":"fine-tuning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
